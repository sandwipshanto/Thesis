# RQ3: Model Vulnerability to Bangla Attacks

**Research Question:** Are all major LLMs vulnerable to Bangla attacks?

---

## Key Findings

### 1. Model Vulnerability Ranking

| Rank | Model | AASR (Overall) | English | CM | CMP | Vulnerability Level |
|------|-------|----------------|---------|----|----|---------------------|
| 1 | **Mistral-7B** | **81.8%** | 84.1% | 80.0% | 81.3% | **CRITICAL** |
| 2 | Llama-3-8B | 22.7% | 11.6% | 25.6% | 30.9% | MODERATE |
| 3 | GPT-4o-mini | 16.0% | 1.5% | 20.7% | 25.7% | LOW (but exploitable) |

**Key Finding:** All models tested show vulnerability, but severity varies dramatically.

### 2. Mistral-7B: Universally Vulnerable

**Mistral-7B shows catastrophic safety failure:**

- ‚úÖ **84.1% AASR on English baseline** (critically vulnerable without any attack!)
- ‚ö†Ô∏è **80.0% AASR with code-mixing** (-4.1 pp - slight decrease)
- ‚ö†Ô∏è **81.3% AASR with phonetic perturbations** (-2.8 pp total)

**Analysis:** Mistral-7B's safety filters are fundamentally broken - it generates harmful content even for straightforward English prompts (84.1%). Code-mixing actually slightly reduces effectiveness, likely due to language understanding degradation.

**Implication:** Mistral-7B requires urgent safety overhaul before any deployment. The model is vulnerable regardless of language or attack strategy.

### 3. GPT-4o-mini: Code-Mixing Vulnerable

**GPT-4o-mini shows strong English safety but Bangla vulnerability:**

- ‚úÖ **1.5% AASR on English** (strong baseline)
- ‚ùå **20.7% AASR with CM** (+19.2 pp - 14√ó increase!)
- ‚ùå **25.7% AASR with CMP** (+24.2 pp - 17√ó increase!)

**Analysis:** GPT-4o-mini's safety training is English-centric. Code-mixing significantly bypasses filters, with phonetic perturbations providing additional 5pp gain.

**Implication:** OpenAI's safety alignment does not generalize to Bangla-English code-mixing.

### 4. Llama-3-8B: Consistent Vulnerability

**Llama-3-8B shows progressive vulnerability increase:**

- ‚úÖ **11.6% AASR on English** (moderate baseline)
- ‚ùå **25.6% AASR with CM** (+14.0 pp - 2.2√ó increase)
- ‚ùå **30.9% AASR with CMP** (+19.3 pp - 2.7√ó increase)

**Analysis:** Llama-3-8B shows the expected English‚ÜíCM‚ÜíCMP progression, with code-mixing nearly tripling attack success rates. The model is vulnerable to both language mixing and phonetic perturbations.

**Implication:** Llama-3-8B's safety training lacks coverage for multilingual code-mixing scenarios.

---

## Template-Model Interaction

### None Template (Most Effective Overall)

| Model | None + English | None + CM | None + CMP |
|-------|----------------|-----------|------------|
| GPT-4o-mini | 1.3% | 60.0% | **70.7%** |
| Llama-3-8B | 8.0% | 47.3% | 52.0% |
| Mistral-7B | 54.7% | 57.3% | 64.7% |

**Key Insight:** Simple prompts without jailbreak templates are most effective when combined with code-mixing (except for already-vulnerable models).

### Jailbreak Template Effectiveness

**Surprisingly, jailbreak templates REDUCE effectiveness for Bangla:**

| Template | GPT-4o-mini AASR | Llama-3-8B AASR | Mistral-7B AASR |
|----------|------------------|-----------------|-----------------|  
| None | 29.3% | 33.0% | 59.6% |
| AntiLM | 1.6% | 42.2% | 81.3% |
| OM | 24.4% | 11.1% | 81.8% |
| AIM | 3.6% | 14.7% | 91.6% |
| Sandbox | 3.6% | 11.6% | 90.2% |**Finding:** Template effectiveness varies by model. For GPT-4o-mini and Llama-3-8B, "None" template is most effective (29.3% and 33.0% respectively). For Mistral-7B, jailbreak templates like AIM (91.6%) and Sandbox (90.2%) are most effective, though the model is highly vulnerable regardless.

---

## Answer to RQ3

**Yes, all major LLMs tested are vulnerable to Bangla attacks, but with different severity levels:**

1. ‚úÖ **Mistral-7B:** Catastrophically vulnerable (81.8% overall AASR, 84.1% on English baseline)
2. ‚úÖ **Llama-3-8B:** Moderately vulnerable (22.7% overall, 30.9% with CMP - 2.7√ó English baseline)
3. ‚úÖ **GPT-4o-mini:** Lower vulnerability but significant gain via code-mixing (16.0% overall, 25.7% with CMP - 17√ó English baseline)

**Critical Discovery:** Template effectiveness varies by model. "None" template most effective for GPT-4o-mini (29.3%) and Llama-3-8B (33.0%), while Mistral-7B shows high vulnerability across all templates.

---

## Safety Coverage Analysis

### What This Reveals About Model Safety

**GPT-4o-mini (OpenAI):**
- ‚úÖ Strong English safety (1.3% AASR)
- ‚ùå No Bangla safety coverage (22.4% AASR with CM)
- ‚ùå Tokenization-based filters ineffective for Banglish

**Llama-3-8B (Meta):**
- ‚ùå Moderate baseline vulnerability (22.7% AASR)
- ‚úÖ Language-agnostic safety (same AASR across languages)
- ‚ùå Overall safety alignment needs improvement

**Mistral-7B (Mistral AI):**
- ‚ùå Critical safety failure (73.1% English baseline)
- ‚ùå No effective safety filters for any language
- üö® **Urgent action required**

---

## Implications for Bangla Safety

1. **No model is adequately protected** against Bangla code-mixing attacks
2. **English-centric safety training** fails to generalize to multilingual contexts
3. **Tokenization-based detection** is insufficient for code-mixed text
4. **Semantic understanding needed** - models must recognize harmful intent regardless of language mixing

---

## Recommendations

### For Model Developers

**OpenAI (GPT-4o-mini):**
- Add Bangla-English code-mixing examples to safety training
- Implement semantic-level harm detection (not token-based)
- Test safety alignment on Indic language code-mixing

**Meta (Llama-3-8B):**
- Strengthen baseline safety alignment
- Ensure consistency across monolingual and code-mixed inputs
- Expand red-teaming to include low-resource languages

**Mistral AI (Mistral-7B):**
- üö® **Urgent:** Implement basic safety filters
- Conduct comprehensive safety evaluation before deployment
- Consider model withdrawal until safety improves

---

## Limitations

- **Model size:** Only tested 7-8B models (larger models may have better safety)
- **Model versions:** Results apply to specific versions tested (may improve with updates)
- **Limited models:** Did not test Gemini, Claude, or other major LLMs
- **Single language pair:** Cannot generalize to other Indic languages

---

**Status:** ‚úÖ RQ3 Confirmed - All models vulnerable, Mistral-7B critically so
