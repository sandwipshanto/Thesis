\chapter{Discussion}
\label{ch:discussion}

This chapter interprets our findings, compares them with related work, explores implications for LLM safety, and addresses methodological considerations.

\section{Principal Findings}
\label{sec:principal-findings}

Our study provides the first comprehensive evaluation of Bangla-English code-mixing attacks on LLMs, yielding four major findings:

\subsection{Finding 1: Bangla Code-Mixing is Effective}
Our results demonstrate that Bangla-English code-mixing constitutes a meaningful attack surface against LLM safety filters, achieving 46\% AASR with phonetically perturbed prompts. This represents a 42\% improvement over the English baseline, demonstrating genuine vulnerability rather than marginal exploitation. Statistical significance at $p < 0.001$ confirms the robustness of this finding across our experimental conditions.

\subsection{Finding 2: English Word Targeting is Optimal}
A critical discovery of our research is that targeting English words for phonetic perturbation proves 68\% more effective than perturbing Bangla words within code-mixed prompts. This finding aligns with the hypothesis that safety filters remain primarily English-centric in their training and pattern matching. Importantly, this represents a novel contribution not systematically explored in prior code-mixing attack research.

\subsection{Finding 3: Inconsistent Model Vulnerability}
Our cross-model analysis reveals dramatic inconsistency in vulnerability to Bangla attacks. Mistral-7B proves critically compromised at 81.8\% average AASR, while GPT-4o-mini demonstrates the strongest resistance at 16.0\% average AASR yet remains exploitable with a 17$\times$ increase over its English baseline. Critically, no tested model achieves adequate safety coverage for the 230 million Bangla speakers worldwide, exposing a systemic gap in multilingual AI safety.

\subsection{Finding 4: Tokenization is the Primary Mechanism}
Our tokenization analysis confirms that subword fragmentation serves as the primary attack mechanism for Bangla code-mixing exploits. The observed AASR progression patterns align with the tokenization disruption mechanism previously validated for Hindi-English attacks. Phonetic perturbations systematically fragment harmful keywords into semantically inert subword units, enabling evasion of token-level safety filters through deliberate disruption of the pattern-matching substrate.

\section{Comparison with Related Work}
\label{sec:comparison}

\subsection{Hinglish Code-Mixing Study}

Our work was inspired by \citet{aswal2025}. Key comparisons:

\textbf{Methodological Similarities:} Our experimental design deliberately parallels the Hinglish study in several key respects. Both employ a three-step transformation pipeline (English $\rightarrow$ CM $\rightarrow$ CMP), test the same model architectures (GPT-4o-mini, Llama-3-8B, Mistral-7B), investigate the tokenization fragmentation hypothesis, and utilize LLM-as-judge evaluation for scalable response assessment.

Critical Differences: Despite methodological parallels, our work constitutes an independent contribution with several distinguishing features. We investigate Bangla rather than Hindi, which differs substantially in phonology and romanization conventions. Our dataset comprises 200 carefully curated prompts compared to their 460 prompts, reflecting budget constraints that limited us to testing 3 models fully. While smaller in scale, our study yields novel findings including the English-word targeting strategy and the counterintuitive ineffectiveness of jailbreak templates for code-mixing attacks.

\textbf{Effectiveness Comparison:} The Hinglish study reported 99\% AASR with their CMP variant, while our Bangla CMP achieves 46\% AASR. However, these figures are not directly comparable due to different experimental conditions, including distinct prompt sets, evaluation criteria, and language-specific characteristics. The absolute effectiveness difference likely reflects a combination of linguistic variation, dataset composition, and methodological choices rather than inherent superiority of one attack language over another.

\subsection{Multilingual Safety Studies}

\textbf{\citet{deng2023}:} Their multilingual jailbreaking study tested 6 languages and found consistently higher jailbreak rates for non-English languages. Our work extends this pattern to Indic languages specifically, providing the first systematic evaluation of Bangla vulnerability and confirming that the multilingual safety gap they identified applies broadly to South Asian language communities.

\textbf{\citet{yong2023}:} Their study found 25-40\% higher toxic outputs for low-resource languages compared to English. Our work extends these findings by quantifying the specific vulnerability for Bangla, demonstrating 46\% AASR with code-mixed and perturbed prompts, thereby providing empirical evidence for one of the world's most widely spoken yet under-resourced languages in the context of LLM safety.

\section{Implications for LLM Safety}
\label{sec:implications}

\subsection{Multilingual Safety Gaps}

Our findings reveal three critical gaps in current LLM safety approaches. First, we observe language-specific vulnerabilities where safety training fails to generalize to Bangla-English code-mixing contexts, suggesting that alignment procedures remain narrowly focused on monolingual English scenarios. Second, we demonstrate tokenization brittleness in which token-level safety filters are easily evaded through phonetic perturbations that fragment harmful keywords into semantically inert subword units. Third, these technical limitations manifest as inequitable protection, leaving 230 million Bangla speakers with demonstrably inadequate safety coverage compared to their English-speaking counterparts.

\subsection{Recommendations for Model Developers}

\textbf{Short-term mitigations:} Model developers should immediately incorporate code-mixed text into their red-teaming efforts to proactively identify multilingual vulnerabilities before deployment. RLHF datasets must be expanded to systematically cover Bangla and other Indic languages, ensuring that safety alignment training reflects the linguistic diversity of global user populations. Additionally, safety architectures should transition from pure token-level pattern matching to semantic-level safety checks that evaluate harmful intent independent of surface-form variations.

\textbf{Long-term solutions:} Fundamental architectural changes are necessary to address the root causes of multilingual vulnerability. Safety mechanisms must be redesigned to operate independently of tokenization schemes, potentially through embedding-space classifiers or character-level approaches that resist fragmentation attacks. Multilingual safety classifiers should be trained with explicit representation of code-mixing phenomena, enabling cross-lingual transfer of safety knowledge. Finally, dynamic romanization normalization systems should be developed to canonicalize the diverse romanization schemes used in informal digital communication, reducing the attack surface created by spelling variation.

\subsection{Policy Considerations}

Policy interventions are necessary to ensure equitable AI safety at scale. Language coverage requirements should be established mandating that safety standards apply to all major languages with speaker populations exceeding 100 million, preventing the current practice of English-only safety guarantees. Transparency mechanisms must be strengthened, requiring model cards to explicitly disclose known vulnerabilities by language so that users and downstream developers can make informed deployment decisions. Regional deployment policies should enforce higher safety thresholds in geographic areas where language-specific vulnerabilities have been identified, preventing the export of inadequately tested systems to vulnerable populations.

\section{Unexpected Findings}
\label{sec:unexpected}

\subsection{Jailbreak Templates Reduce Effectiveness}

Contrary to prior work \citep{aswal2025}, jailbreak templates \textbf{reduced} Bangla attack effectiveness:

\textbf{Possible explanations:} We propose four potential mechanisms for this counterintuitive finding. First, jailbreak templates may trigger additional safety checks due to their distinctive patterns, which are now well-documented in adversarial literature and likely incorporated into model training. Second, code-mixing alone may provide sufficient obfuscation to bypass filters, rendering the additional complexity of templates unnecessary and potentially counterproductive. Third, models have likely been explicitly trained to detect common jailbreak template patterns through adversarial fine-tuning, making templates actually increase rather than decrease detection likelihood. Fourth, language-specific interaction effects between code-mixing and template structures may create unforeseen detection signals absent in monolingual contexts.

\textbf{Implication:} Simple, direct prompts prove most effective for Bangla attacks, suggesting that adversaries targeting Bangla speakers need not employ sophisticated prompt engineering techniques.

\subsection{Mistral's Critical Vulnerability}

Mistral-7B's 81.8\% baseline vulnerability is unexpected:

\textbf{Potential causes:} Three factors may contribute to Mistral-7B's disproportionate vulnerability. First, as a relatively recent open-source release, the model may have received insufficient safety fine-tuning compared to commercial alternatives that benefit from extensive RLHF and red-teaming resources. Second, training data imbalance likely emphasizes European languages and contexts given Mistral AI's French origins, potentially underrepresenting South Asian linguistic patterns and safety scenarios. Third, the model's safety training may reflect European regulatory priorities and threat models, systematically missing South Asian cultural and linguistic contexts that would enable detection of Bangla-encoded harmful content.

\textbf{Implication:} Open-source models require robust community-driven safety improvements to achieve parity with commercial alternatives, particularly for language communities underrepresented in proprietary training data.

\section{Limitations and Future Work}
\label{sec:discussion-limitations}

\subsection{Study Limitations}

\begin{enumerate}
    \item Dataset size: 200 prompts (scaled from 50) vs. 460 in prior work
    \item Model coverage: 3 models fully tested
    \item Temperature settings: 3 values vs. 6 in full factorial design
    \item Manual code-mixing: Time-intensive, not fully automated
\end{enumerate}

\subsection{Future Research Directions}

\begin{itemize}
    \item \textbf{Scale to 460 prompts:} Full replication of Hinglish study
    \item \textbf{Automated code-mixing:} NMT-based Bangla-English generation
    \item \textbf{Other Indic languages:} Tamil, Telugu, Marathi (20+ languages)
    \item \textbf{Defense mechanisms:} Develop Bangla-aware safety filters
    \item \textbf{Human evaluation:} Validate LLM-as-judge with ICC study
\end{itemize}

\section{Methodological Contributions}
\label{sec:method-contributions}

\subsection{Scalable Framework}

Our methodology is replicable for other languages:

\textbf{Cost per language:} \$2.00-2.50 (200 prompts), \$0.50 (50-prompt validation)

\textbf{Applicable to:} This framework can be directly applied to assess vulnerabilities in other major Indic languages, including Tamil with 75 million speakers, Telugu with 82 million speakers, Marathi with 83 million speakers, Urdu with 70 million speakers, and Gujarati with 56 million speakers, among many others. The collective speaker population of these languages exceeds 400 million individuals, all of whom potentially face similar safety gaps in current LLM deployments.

\subsection{Config-Driven Experimentation}

\textbf{Key innovation:} \texttt{run\_config.yaml} controls all experiments

\textbf{Benefits:} This configuration-driven approach offers several methodological advantages. Researchers can modify experimental parameters without code changes, reducing implementation errors and accelerating iteration cycles. Parameter sweeps become trivial to execute through simple YAML edits, enabling comprehensive sensitivity analysis. Experimental configurations are fully reproducible through version-controlled config files, addressing a persistent challenge in computational research. Most importantly, the approach substantially lowers barriers to replication, enabling researchers without deep programming expertise to adapt the framework for new languages or attack strategies.

\section{Summary}
\label{sec:discussion-summary}

Our discussion establishes six core findings that advance understanding of multilingual LLM safety. First, Bangla code-mixing constitutes an effective jailbreaking strategy, achieving 46\% AASR compared to the 32\% English baseline. Second, English word targeting proves optimal for Bangla attacks, demonstrating 68\% higher effectiveness than Bangla word perturbations due to English-centric safety filter design. Third, all major tested LLMs exhibit vulnerability to Bangla attacks, though vulnerability magnitude varies dramatically across models (16\% to 82\% AASR). Fourth, the tokenization fragmentation mechanism empirically validated for Hindi-English code-mixing attacks \citep{aswal2025} applies equally to Bangla-English contexts, with observed AASR progression aligning with token fragmentation patterns. Fifth, current safety alignment approaches fail to generalize from English to Bangla-English code-mixing scenarios, exposing a fundamental limitation in existing RLHF methodologies. Sixth, urgent improvements are needed in multilingual safety training to address systematic gaps affecting hundreds of millions of non-English speakers worldwide.

These findings have important implications across four key stakeholder groups. For LLM developers, our results necessitate fundamental changes to safety training practices, including incorporation of code-mixed adversarial examples and expansion of RLHF coverage beyond predominantly English datasets. Policy makers must establish language coverage requirements to ensure equitable protection, moving beyond informal best practices to enforceable standards. The research community gains a replicable framework applicable to dozens of other low-resource languages, enabling systematic multilingual vulnerability assessment at modest cost. Most importantly, our findings directly benefit the 230 million Bangla speakers worldwide who currently receive inadequate safety protection, providing evidence-based advocacy for their right to equitable AI safety coverage.
