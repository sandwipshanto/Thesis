\chapter*{ABSTRACT}
\addcontentsline{toc}{chapter}{Abstract}

Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial attacks, particularly in multilingual contexts. While existing research has demonstrated vulnerabilities in English and Hindi-English (Hinglish) code-mixing, no prior work has examined Bangla-English (Banglish) code-mixing attacks despite Bangla being the 8th most spoken language globally with 230 million native speakers.

This thesis presents the \textbf{first comprehensive study} of Bangla-English code-mixing combined with phonetic perturbations as a jailbreaking strategy against modern LLMs. We develop a systematic three-step methodology: (1) converting harmful queries to hypothetical scenarios, (2) code-mixing with romanized Bangla, and (3) applying phonetic perturbations to sensitive English keywords.

Through systematic experiments across 3 major LLMs (GPT-4o-mini, Llama-3-8B, Mistral-7B) using 50 harmful prompts across 10 categories (reduced from planned 460 due to budget constraints), we generated approximately 6,750 model responses evaluated through automated LLM-as-judge methodology. Our results demonstrate that Bangla code-mixing with phonetic perturbations achieves \textbf{46\% Average Attack Success Rate (AASR)}, representing a \textbf{42\% improvement} over the 32.4\% English baseline.

This research makes several novel contributions to multilingual LLM security. First, it presents the first systematic investigation of Bangla-English code-mixed jailbreaking attacks, addressing a vulnerability affecting 230 million speakers previously untested in adversarial contexts through experiments with 50 prompts across 10 harmful categories using 3 major LLMs. Second, we discover that perturbing English words within Banglish contexts is 68\% more effective than perturbing Bangla words, revealing language-specific targeting strategies. Third, we find that jailbreak templates counterintuitively reduce attack effectiveness for Bangla, with simple prompts outperforming sophisticated jailbreak frameworks. Fourth, we apply the tokenization disruption mechanism empirically validated for Hindi-English by Aswal and Jaiswal (2025) to the Bangla-English context, demonstrating consistent AASR patterns aligned with token fragmentation progression. Fifth, we identify Bangla's non-standard romanization as a unique vulnerability creating multiple valid tokenization paths that evade safety filters. Finally, we develop a scalable experimental framework applicable to 20+ other Indic languages at approximately \$1.50-2.00 per language, enabling broader multilingual security research.

\noindent\textbf{Implications:} This research reveals critical gaps in multilingual LLM safety, particularly for low-resource Indic languages. Our findings demonstrate that current safety alignment fails to generalize to Bangla-English code-mixing, necessitating urgent improvements in multilingual safety training and tokenization-robust detection systems.

\vspace{0.5cm}

\noindent\textbf{Keywords:} Large Language Models, Jailbreaking, Code-Mixing, Bangla, Adversarial Attacks, LLM Safety, Multilingual NLP, Phonetic Perturbations, Tokenization
