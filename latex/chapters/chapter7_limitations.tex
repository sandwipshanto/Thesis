\chapter{Limitations}
\label{ch:limitations}

This chapter acknowledges the limitations of our study and discusses their implications for the interpretation and generalizability of our findings.

\section{Dataset Limitations}
\label{sec:dataset-limitations}

\subsection{Limited Prompt Count}

\textbf{Limitation:} Our study employs 200 prompts compared to 460 prompts in the Hinglish reference study \citep{aswal2025}, representing an approximately 57\% reduction driven by budget constraints (approximately \$2.50 available versus \$5-10 required for full-scale execution). This dataset size resulted from iterative scaling that balanced initial validation at 50 prompts with expanded statistical power at 200 prompts. Additionally, experimental interruptions limited data collection to approximately 75\% completion, yielding 27,000 of the planned 36,000 total queries across all experimental configurations.

\textbf{Impact:} Despite the reduced dataset size, our results remain highly statistically significant, with the primary English-to-CMP transition achieving p=0.0070 significance. The balanced 20-prompts-per-category distribution ensures adequate representation of category-specific attack patterns, while the 27,000-query sample size provides sufficient statistical power for publication-quality findings that meet or exceed standards in adversarial ML research.

\textbf{Mitigation:} Several design choices mitigate the impact of reduced dataset size. The balanced distribution across 10 harm categories with 20 prompts each represents a fourfold increase from initial validation, ensuring robust category coverage. The highly significant results achieved (p=0.0070 for the main English-to-CMP transition) demonstrate adequate statistical power despite scale limitations. The framework has been validated through iterative scaling from 50 to 200 prompts, establishing a clear path for future expansion to the full 460-prompt target. Additionally, the implementation includes resume functionality enabling completion of the remaining 9,000 queries if additional resources become available, preserving all experimental infrastructure for seamless continuation.

\subsection{Manual Code-Mixing}

\textbf{Limitation:} Code-mixing was performed manually by the authors rather than through automated generation, introducing potential for subjective variation in linguistic choices and romanization conventions. This manual process proved time-intensive, creating practical scalability constraints that would limit extension to thousands of prompts without substantial human effort investment.

\textbf{Impact:} The quality and consistency of code-mixing outputs depend fundamentally on the authors' Bangla proficiency and intuitions about naturalistic code-mixing patterns. Different researchers replicating this methodology might produce variant code-mixed prompts reflecting their own linguistic backgrounds and preferences, potentially affecting attack effectiveness in unpredictable ways. This manual dependency makes scaling to thousands of prompts practically infeasible without automated generation tools or substantial expansion of the research team.

\textbf{Mitigation:} Several factors partially address these concerns. The authors are native Bangla speakers with natural competence in code-mixing practices common among bilingual Bangla-English users in digital contexts. Manual review and consistency checks were performed across all code-mixed prompts to ensure comparable complexity and romanization conventions. For future work, we plan to develop automated NMT-based code-mixing generation systems that can scale to thousands of prompts while maintaining the linguistic naturalness validated through our manual approach.

\section{Model Coverage Limitations}
\label{sec:model-limitations}

\subsection{Limited Model Selection}

\textbf{Limitation:}
\begin{itemize}
    \item Only 3 models fully tested (GPT-4o-mini, Llama-3-8B, Mistral-7B)
    \item Gemma-1.1-7B excluded due to budget constraints
    \item Missing major models: Claude, PaLM 2, newer GPT-4
    \item Open-source models limited to 7-8B parameters
\end{itemize}

\textbf{Impact:}
\begin{itemize}
    \item Results may not generalize to all LLM architectures
    \item Google's LLM safety approach not evaluated (Gemma missing)
    \item Larger models (70B+) might have different vulnerabilities
    \item Proprietary models like Claude not evaluated
\end{itemize}

\textbf{Rationale:}
\begin{itemize}
    \item Budget constraints ($\sim$\$2.50 total spending: \$0.38 validation + \$2.12 for 200-prompt scale-up)
    \item Full scale would have required $\sim$\$10 (460 prompts, 4 models)
    \item OpenRouter API access limitations
    \item Prioritized depth over breadth given constraints
\end{itemize}

\subsection{Model Version Stability}

\textbf{Limitation:} API-accessed models may be silently updated by providers during our study period, with no guarantees of version stability. Model providers regularly deploy safety patches and capability improvements, meaning the exact model weights evaluated may change mid-study without notification to API users.

\textbf{Impact:} This version instability creates reproducibility challenges, as future researchers cannot access identical model versions for verification. Our results may not hold for future versions if safety improvements specifically target code-mixing vulnerabilities. The temporal validity of findings is inherently limited to the snapshot period of evaluation (November-December 2024), with diminishing relevance as models continue evolving.

\section{Experimental Design Limitations}
\label{sec:experimental-limitations}

\subsection{Temperature Settings}

\textbf{Limitation:} Our experimental design tested only three temperature values (0.2, 0.6, 1.0), representing low, medium, and high randomness settings, whereas the original Hinglish study employed six temperature values for finer-grained analysis. This coarser sampling may miss optimal temperature ranges for maximizing attack effectiveness.

\textbf{Impact:} The temperature sensitivity analysis provides less comprehensive coverage than ideal, potentially overlooking peak attack effectiveness at intermediate values like 0.4 or 0.8. The reduced granularity limits our ability to precisely characterize temperature-attack relationships, though our results suggest temperature effects are relatively modest compared to prompt transformation effects.

\textbf{Rationale:} This design choice reduced total query count by 50\%, generating substantial cost savings (approximately \$1.25 reduction per 200-prompt run). The three selected values adequately capture the spectrum from deterministic (0.2) to highly random (1.0) generation. Post-hoc analysis confirms that temperature effects remain secondary to prompt set variations, suggesting the coarser sampling did not critically compromise findings.

\subsection{Single-Turn Evaluation}

\textbf{Limitation:} Our evaluation framework assesses only single-turn attacks where each prompt is evaluated independently without conversational context. Multi-turn strategies where adversaries gradually build trust, establish fictional scenarios, or incrementally escalate requests remain unexplored. Context accumulation effects across conversation turns are not measured.

\textbf{Impact:} This single-turn focus may systematically underestimate true attack effectiveness, as real-world adversarial interactions frequently employ multi-turn strategies to circumvent safety filters. Conversational jailbreaking techniques that exploit dialogue state management and context window limitations are not captured by our methodology. However, single-turn attacks represent a conservative lower bound on vulnerability, suggesting our findings indicate minimum rather than maximum risk levels.

\section{Evaluation Limitations}
\label{sec:evaluation-limitations}

\subsection{LLM-as-Judge Reliability}

\textbf{Limitation:} We employ GPT-4o-mini as the sole evaluator without independent human validation, meaning inter-coder reliability (ICC) against human annotators remains uncalculated for our specific dataset. This introduces potential for systematic biases in the judge model's interpretation of harmfulness and relevance, particularly for code-mixed content that may challenge its training distribution.

\textbf{Impact:} Evaluation accuracy fundamentally depends on the judge model's ability to correctly identify harmful content across linguistic variations. The automated approach may miss subtle harmful implications that human evaluators would detect, or conversely may flag benign content as harmful. Language-specific biases could systematically over- or under-estimate attack success for Bangla compared to English baselines.

\textbf{Mitigation:} Prior research validates LLM-as-judge approaches with ICC $\geq$ 0.70 against human annotators for similar harmful content classification tasks \citep{aswal2025}, providing methodological precedent. We maintain consistent evaluation criteria across all 27,000 responses, ensuring comparative validity even if absolute accuracy varies. Future work will include human annotation of a random 100-response sample to empirically validate judge reliability for Bangla-specific content.

\subsection{Binary Harmfulness Classification}

\textbf{Limitation:} Our success metric employs binary classification (harmful/harmless) rather than continuous harm severity scoring. This approach does not differentiate between responses providing vague suggestions versus detailed step-by-step instructions, potentially oversimplifying the nuanced spectrum of harmful content generation.

\textbf{Impact:} The binary approach loses granularity in harm severity assessment, treating partial information disclosure (e.g., mentioning general concepts) identically to comprehensive actionable instructions. Threshold effects where specific prompt modifications shift responses from borderline to clearly harmful remain unexplored. This may obscure important gradations in model safety robustness, though it ensures conservative estimates where any harmful disclosure counts as attack success.

\section{Linguistic Limitations}
\label{sec:linguistic-limitations}

\subsection{Romanization Variability}

\textbf{Limitation:} We applied no systematic romanization standard (such as ISO 15919 or National Romanization), instead relying on the authors' intuitive romanization practices reflecting informal digital communication norms. This approach may not represent the full diversity of romanization conventions used by Bangla speakers across different regions, educational backgrounds, and age groups. Regional dialectal variations and their romanization patterns remain unexplored.

\textbf{Impact:} Attack effectiveness may vary substantially with alternative romanization schemes, limiting generalizability of our specific AASR values to all Banglish variants in real-world usage. Different romanization conventions could produce different tokenization patterns, potentially altering attack success rates. However, this limitation also suggests our findings represent one attack variant among many possible romanization-based approaches, indicating the vulnerability space may be even larger than documented.

\subsection{Single Language Pair}

\textbf{Limitation:} Our investigation focuses exclusively on Bangla-English code-mixing without evaluating other major Indic languages such as Tamil, Telugu, Marathi, or Urdu. This single-language approach prevents establishment of cross-linguistic attack patterns that distinguish language-specific vulnerabilities from generalizable code-mixing phenomena.

\textbf{Impact:} We cannot empirically confirm whether observed patterns generalize to other romanized Indic languages or represent Bangla-specific characteristics. The relative contributions of Bangla-specific linguistic features (such as non-standardized romanization) versus universal code-mixing effects remain unclear without comparative analysis. However, linguistic similarities across Indic language families suggest our findings likely indicate broader vulnerability patterns, making this a foundation for future comparative work rather than a fatal limitation.

\section{Interpretability Limitations}
\label{sec:interpretability-limitations}

\subsection{Tokenization Analysis}

\textbf{Limitation:} Our mechanistic understanding relies on observational evidence showing AASR progression aligning with tokenization fragmentation patterns, consistent with findings from Hindi-English research \citep{aswal2025}. However, we did not conduct direct empirical validation through Integrated Gradients or similar attribution methods specifically for Bangla. This means we infer the causal mechanism from pattern correlation rather than proving it through model internals analysis.

\textbf{Impact:} The tokenization disruption hypothesis, while strongly supported by observational patterns and prior empirical work on Hindi, remains incompletely validated for Bangla specifically. Other contributing factors such as training data distribution, semantic encoding differences, or safety filter architecture details may partially explain attack success. Attribution analysis through techniques like Integrated Gradients or attention visualization would strengthen causal claims, though the strong correlation with established mechanisms provides substantial indirect validation.

\subsection{Black-Box Evaluation}

\textbf{Limitation:} API-only access to tested models prevents inspection of internal mechanisms, attention patterns, or safety filter architectures. We cannot directly observe model internals to verify hypothesized mechanisms, relying instead on behavioral evidence from input-output patterns.

\textbf{Impact:} This black-box constraint limits mechanistic understanding to inference from external behavior. We cannot inspect attention weights to confirm that code-mixed text receives different processing than English, nor can we verify tokenization hypothesis by examining internal representations. The analysis remains fundamentally behavioral rather than mechanistic, though systematic behavioral patterns provide substantial indirect evidence for proposed mechanisms.

\section{Ethical and Practical Limitations}
\label{sec:ethical-limitations}

\subsection{Budget Constraints}

\textbf{Limitation:}
\begin{itemize}
    \item Total budget: $\sim$\$1 (very limited for academic research)
    \item Prevented full 460-prompt dataset execution
    \item Limited to 200 prompts ($\sim$57\% reduction from planned 460-prompt full scale)
    \item Gemma-1.1-7B excluded to stay within budget
\end{itemize}

\textbf{Impact:}
\begin{itemize}
    \item 27,000 responses collected (75\% of planned 36,000 for full 200-prompt factorial design)
    \item Only 3 models tested instead of 4
    \item Reduced statistical power and generalizability
    \item Cannot afford extensive parameter exploration
\end{itemize}

\textbf{Context:}
\begin{itemize}
    \item Full-scale study (460 prompts, 4 models) would have cost $\sim$\$10
    \item Undergraduate research with limited institutional funding
    \item Methodology remains sound despite reduced scale
\end{itemize}

\subsection{Responsible Disclosure Timing}

\textbf{Limitation:} Our findings are disclosed initially in an academic thesis context rather than through pre-publication notification to affected model developers. While this follows academic norms, it means vendors receive vulnerability information simultaneously with or after academic review rather than through advance confidential disclosure. The full prompt dataset is intentionally not released publicly to mitigate immediate weaponization risks.

\textbf{Impact:} Documented vulnerabilities remain unpatched at initial publication time, creating a window during which malicious actors aware of our findings could potentially exploit identified weaknesses before vendors deploy fixes. The public academic disclosure may increase jailbreaking attempt frequency as adversaries apply our methodology. However, the delayed vendor notification is partially offset by our decision to withhold the full attack dataset.

\textbf{Mitigation:} We implement three protective measures to minimize exploitation risks. The complete harmful prompt dataset and model responses are not publicly released, requiring formal research agreements for access. Responsible disclosure to all affected vendors is planned immediately following thesis submission, providing detailed technical findings to accelerate patch development. Methodology sharing remains restricted to research contexts rather than providing turnkey exploitation tools.

\section{Generalizability Limitations}
\label{sec:generalizability}

\subsection{Temporal Validity}

\textbf{Limitation:} Our evaluation represents a temporal snapshot conducted during November-December 2024, capturing model behavior at a specific point in their continuous evolution. LLM providers regularly update models with safety improvements, capability enhancements, and architectural changes, meaning safety characteristics evolve rapidly over time.

\textbf{Impact:} Findings may not persist for future model versions, particularly if vendors specifically address code-mixing vulnerabilities in response to our disclosure or independent discovery. Documented attacks may be patched through updated safety filters, additional RLHF training, or architectural changes, reducing attack success rates over time. Our results thus provide historical validity for the tested model versions but cannot guarantee persistence across future deployments, though they establish baseline vulnerability levels for comparative assessment.

\subsection{Real-World Applicability}

\textbf{Limitation:} Our evaluation occurs in a controlled research environment with explicit adversarial intent, differing substantially from typical user interactions with LLM systems. The experimental setting assumes sophisticated adversaries deliberately crafting attacks, which may not reflect organic user behavior patterns or accidental harmful content generation.

\textbf{Impact:} Actual exploitation rates in real-world deployments may differ significantly from laboratory attack success rates. User behavior factors including linguistic competence, motivation levels, and awareness of adversarial techniques are not modeled in our purely technical evaluation. Platform-level mitigations such as rate limiting, user reputation systems, or human-in-the-loop review processes deployed in production environments are not accounted for in our direct API testing framework. However, our findings establish technical feasibility even if real-world exploitation requires additional factors to align.

\section{Summary}
\label{sec:limitations-summary}

Despite these limitations, our study provides valuable insights into Bangla-specific LLM vulnerabilities and establishes a foundation for multilingual safety research.

\textbf{Key Strengths:} This work represents the first comprehensive Bangla code-mixing vulnerability study, addressing a critical gap for 230 million speakers. Our findings achieve statistical significance (p=0.0070 for main effects) despite reduced dataset scale, validating robustness of observed patterns. The methodology is fully documented and replicable at modest cost (\$1.50-2.00 per language), enabling community-driven extension. Novel language-specific insights including English word targeting effectiveness and jailbreak template ineffectiveness advance theoretical understanding beyond simple replication of prior work.

\textbf{Acknowledged Weaknesses:} Dataset size remains limited at 200 prompts compared to the 460-prompt Hinglish reference study, reducing statistical power for subgroup analysis. Model coverage is incomplete with only three of four planned models fully tested due to budget constraints excluding Gemma. Human evaluation validation is absent, with LLM-as-judge reliability uncalibrated against human annotators for Bangla-specific content. Analysis remains restricted to black-box behavioral observation without white-box mechanistic validation through model internals inspection.

\textbf{Future Work Directions:} Five priorities emerge for extending this research. Scaling to the full 460-prompt dataset would enable direct quantitative comparison with Hindi-English baselines and increase subgroup analysis power. Human ICC validation against 100 randomly sampled responses would empirically calibrate automated evaluation reliability. Automated code-mixing generation through NMT models would eliminate manual bottlenecks enabling thousand-prompt scales. White-box interpretability analysis using Integrated Gradients or attention visualization would provide mechanistic validation of tokenization hypotheses. Systematic extension to other major Indic languages (Tamil, Telugu, Marathi, Urdu, etc.) would establish cross-linguistic vulnerability patterns.

The limitations outlined in this chapter should be considered when interpreting our results and planning follow-up studies.
