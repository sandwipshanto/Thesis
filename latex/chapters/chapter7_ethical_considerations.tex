\chapter{Ethical Considerations}
\label{ch:ethical}

Here we'll talk about the ethical side of our research, including responsible disclosure, how we handle our dataset, potential misuse, and what this means for society as a whole.

\section{Research Justification}
\label{sec:researchjustification}

Our research is motivated by the critical need to improve AI safety across linguistic communities. While acknowledging the dual-use nature of vulnerability research, we believe the benefits of disclosure significantly outweigh potential risks.

\subsection{AI Safety Motivation}

We're conducting this research with the primary goal of improving AI safety. Identifying vulnerabilities enables vendors to implement fixes, understanding attack mechanisms informs better safety design, documenting languagespecific gaps promotes equitable protection, and academic disclosure advances collective security knowledge.

\subsection{DualUse Dilemma}

We acknowledge our work can be used for both beneficial and harmful purposes. Beneficial applications include enabling LLM developers to improve multilingual safety training, helping researchers develop tokenization robust defenses, supporting policy makers in establishing language coverage requirements, and expanding redteaming methodologies. Potential misuse includes malicious actors exploiting documented vulnerabilities, attack techniques being weaponized before patches are deployed, and codemixing strategies being applied to other languages.

Our position is that disclosure benefits outweigh risks because vulnerabilities are likely already known to sophisticated adversaries, academic transparency accelerates collective defense, responsible disclosure protocols minimize exploitation windows, and dataset restrictions limit easy replication.

\section{Dataset Handling and Disclosure}
\label{sec:datasethandling}

\subsection{Dataset Handling}

The full harmful prompt dataset and model responses are not publicly released. Only aggregated metrics are available in this thesis, along with sanitized sample prompts for illustrative purposes. This approach balances scientific transparency with harm mitigation, preventing direct replication of attacks while enabling verification of our methodology and findings.

\section{Harm Mitigation Strategies}
\label{sec:harmmitigation}

We have implemented comprehensive safeguards to minimize potential harm from our research while maintaining scientific rigor and transparency.

\subsection{Technical Safeguards}

Dataset anonymization ensures all prompts are stripped of personally identifiable information. Response filtering excludes extremely harmful outputs from analysis. Access controls maintain research data on encrypted, accesscontrolled systems. Version control tracks all changes and maintains auditability.

\subsection{Disclosure Limitations}

Prompt abstraction provides examples without full harmful content. Method generalization describes techniques at conceptual levels. Result aggregation prevents disclosure of individual response content, focusing instead on statistical patterns and aggregate metrics.

\section{Research Limitations and Transparency}
\label{sec:limitations}

\subsection{Data Processing Challenges}

During our experimental data collection and processing, baseline experiments (prompts without jailbreak templates) were initially mislabeled with empty strings in the aggregated metrics file, requiring post-hoc correction before final analysis. This labeling error delayed discovery of a critical finding: jailbreak templates reduce rather than enhance attack effectiveness for Bangla code-mixing. While this issue required regenerating summary statistics, it did not compromise data integrityâ€”all 27,000 raw model responses remained intact and accurate. We disclose this processing challenge to maintain transparency about our methodology and to highlight the importance of rigorous data validation in adversarial ML research.

\section{Societal Impact}
\label{sec:societalimpact}

Our research addresses safety inequities affecting 230 million Bangla speakers who currently receive demonstrably weaker safety protection compared to English speakers. The findings provide empirical evidence for technical and policy improvements toward more equitable AI safety across linguistic communities. The scalable methodology developed in this work can be applied to additional underrepresented languages, enabling broader vulnerability assessment and promoting multilingual safety coverage in LLM deployments.

\section{Chapter Summary}
\label{sec:ethicalsummary}

Our research addresses critical AI safety gaps affecting 230 million Bangla speakers while acknowledging the dual-use nature of vulnerability research. We have implemented safeguards including dataset access restrictions, harm mitigation strategies through prompt abstraction and result aggregation, and commitment to academic transparency. The work advances global language justice in AI systems through evidence-based vulnerability assessment, providing actionable insights for improving multilingual safety coverage in LLM deployments.