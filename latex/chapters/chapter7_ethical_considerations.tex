\chapter{Ethical Considerations}
\label{ch:ethical}

Here we'll talk about the ethical side of our research, including responsible disclosure, how we handle our dataset, potential misuse, and what this means for society as a whole.

\section{Research Justification}
\label{sec:researchjustification}

Our research is motivated by the critical need to improve AI safety across linguistic communities. While acknowledging the dual-use nature of vulnerability research, we believe the benefits of disclosure significantly outweigh potential risks.

\subsection{AI Safety Motivation}

We're conducting this research with the primary goal of improving AI safety. Identifying vulnerabilities enables vendors to implement fixes, understanding attack mechanisms informs better safety design, documenting languagespecific gaps promotes equitable protection, and academic disclosure advances collective security knowledge.

\subsection{DualUse Dilemma}

We acknowledge our work can be used for both beneficial and harmful purposes. Beneficial applications include enabling LLM developers to improve multilingual safety training, helping researchers develop tokenization robust defenses, supporting policy makers in establishing language coverage requirements, and expanding redteaming methodologies. Potential misuse includes malicious actors exploiting documented vulnerabilities, attack techniques being weaponized before patches are deployed, and codemixing strategies being applied to other languages.

Our position is that disclosure benefits outweigh risks because vulnerabilities are likely already known to sophisticated adversaries, academic transparency accelerates collective defense, responsible disclosure protocols minimize exploitation windows, and dataset restrictions limit easy replication.

\section{Responsible Disclosure}
\label{sec:responsibledisclosure}

We are committed to responsible disclosure practices that prioritize vendor notification and coordinated vulnerability disclosure while maintaining academic transparency.

\subsection{Vendor Notification Plan}

We commit to notifying affected organizations:

Timeline:
\begin{enumerate}
    \item Pre-publication (November 2024): Thesis submission to university
    \item Post-submission (December 2024): Prepare vulnerability reports
    \item Vendor contact (January 2025): Email security teams at:
    \begin{itemize}
        \item OpenAI (GPT-4o-mini findings)
        \item Meta (Llama-3-8B findings)
        \item Mistral AI (Mistral-7B findings)
    \end{itemize}
    \item Patch window (60-90 days): Allow vendors time to address issues
    \item Public disclosure: Academic publication after patch deployment
\end{enumerate}

Report contents:
\begin{itemize}
    \item Executive summary of findings
    \item Methodology description (without full prompts)
    \item AASR metrics per model
    \item Recommended mitigations
    \item Offer to collaborate on fixes
\end{itemize}

\subsection{Dataset Handling}

Currently, the full harmful prompt dataset and model responses are not publicly released, with only aggregated metrics available in the thesis and sanitized sample prompts provided. Future release plans include researchonly access with datasets available upon request under usage agreements requiring institutional affiliation verification, signed data use agreements, commitment to responsible use, and no redistribution clauses. Public release will occur only after vendor patches are deployed, with a minimum 6month waiting period.

\section{Harm Mitigation Strategies}
\label{sec:harmmitigation}

We have implemented comprehensive safeguards to minimize potential harm from our research while maintaining scientific rigor and transparency.

\subsection{Technical Safeguards}

Dataset anonymization ensures all prompts are stripped of personally identifiable information. Response filtering excludes extremely harmful outputs from analysis. Access controls maintain research data on encrypted, accesscontrolled systems. Version control tracks all changes and maintains auditability.

\subsection{Disclosure Limitations}

Prompt abstraction provides examples without full harmful content. Method generalization describes techniques at conceptual levels. Result aggregation prevents disclosure of individual response content. Timeline coordination ensures publication only occurs after vendor notification.

\section{Societal Impact Considerations}
\label{sec:societalimpact}

Our research directly addresses systemic inequities in AI safety that disproportionately affect nonEnglish speaking communities, with particular focus on advancing language justice in AI systems.

\subsection{Bangla Speaker Equity}

Our research directly addresses safety inequality affecting 230 million Bangla speakers. Currently, Bangla speakers receive demonstrably weaker safety protection compared to English speakers. The research impact provides evidence for policy and technical improvements, with the longterm goal of achieving equal AI safety regardless of language background. Our advocacy role ensures academic findings support community safety rights.

\subsection{Global Language Justice}

Language coverage gaps highlighted by our work reveal systematic bias in LLM safety training. The scalable methodology applies to dozens of additional languages, while resource accessibility through lowcost evaluation enables broader vulnerability assessment. Policy implications from our findings support multilingual safety requirements.

\section{Institutional Review and Compliance}
\label{sec:institutionalreview}

Our research has undergone rigorous ethical review and complies with institutional and international standards for responsible research conduct.

\subsection{Ethics Committee Review}

We submitted our research protocol to the university ethics committee for comprehensive evaluation. The committee assessed DualUse research implications, data handling procedures, disclosure timeline, and potential harm assessment. Based on their recommendations, we extended the vendor notification period, enhanced dataset access controls, emphasized clearer beneficial use cases, and structured a more comprehensive responsible disclosure plan.

\subsection{Regulatory Compliance}

Data protection measures ensure all research data handling complies with local privacy regulations. Academic integrity standards mean research is conducted under university research ethics guidelines. International norms alignment ensures our disclosure approach follows computer security research standards. Professional standards compliance means methods and reporting follow responsible AI research practices.

\section{Future Research Ethics}
\label{sec:futureresearchethics}

Future research extensions will incorporate enhanced community engagement and continuous impact assessment to ensure beneficial outcomes and community-centered approaches.

\subsection{Community Engagement}

Language community consultation will engage Bangla speaking communities in future work. Cultural sensitivity ensures harm definitions are validated across cultural contexts. Participatory research involves community members in research design and evaluation. Benefit sharing makes research outcomes accessible to affected communities.

\subsection{Continuous Assessment}

Impact monitoring tracks how research findings are used postpublication. Misuse detection monitors for inappropriate application of disclosed techniques. Adaptive disclosure adjusts future disclosure practices based on observed impacts. Community feedback maintains channels for affected community input.

\section{Chapter Summary}
\label{sec:ethicalsummary}

Our research works under a responsible disclosure framework that puts AI safety improvement first while minimizing potential harm. We know our findings can be used both ways and have put in place comprehensive safeguards including vendor notification, dataset restrictions, and ethical review processes. The work directly serves the safety interests of 230 million Bangla speakers and advances global language justice in AI systems. Future work will include community engagement and continuous impact assessment to make sure we get good outcomes.