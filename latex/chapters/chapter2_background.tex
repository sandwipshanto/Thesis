\chapter{Background and Related Work}
\label{ch:background}

\section{Large Language Models and Safety Alignment}
\label{sec:llms-safety}

This section provides foundational context on Large Language Models (LLMs) and the techniques employed to align them with human values and safety standards. We begin by tracing the evolution of LLMs from early transformer architectures to contemporary systems, then examine the multi-stage alignment processes designed to prevent harmful outputs, and finally discuss the persistent gaps in these safety mechanisms—particularly for multilingual and code-mixed contexts.

\subsection{Evolution of LLMs}

Large Language Models have evolved from early transformer architectures \citep{vaswani2017} to sophisticated systems capable of multilingual, multimodal understanding. Modern LLMs like GPT-4 \citep{openai2023}, Llama-3 \citep{dubey2024}, Gemini \citep{google2024}, and Mistral \citep{jiang2023} demonstrate impressive capabilities across diverse tasks including natural language understanding and generation, code generation and debugging, mathematical reasoning, multilingual translation, creative content generation, and question answering with summarization.

\subsection{Safety Alignment Techniques}

To ensure LLMs behave safely and ethically, developers employ multi-stage alignment processes:

\subsubsection{Supervised Fine-Tuning (SFT)}
Supervised fine-tuning involves training models on human-curated examples of safe responses to demonstrate desired behavior patterns while ensuring coverage of harmful query categories that the model must learn to refuse appropriately.

\subsubsection{Reinforcement Learning from Human Feedback (RLHF)}
Reinforcement learning from human feedback operates by having human labelers rank model responses according to safety and quality metrics. These rankings train reward models that learn human preferences, which then guide policy optimization through algorithms such as Proximal Policy Optimization (PPO).

\subsubsection{Constitutional AI}
Constitutional AI enables models to perform self-critique and revision of their own responses, aligning outputs to explicit safety principles without requiring human feedback at inference time. This approach reduces harmful outputs through automated adherence to predefined constitutional rules.

\subsubsection{Red-Teaming}
Red-teaming employs adversarial testing to systematically identify safety failures in deployed models. Through iterative improvement cycles, safety mechanisms are strengthened and alignment robustness is comprehensively evaluated against diverse attack strategies.

Despite these efforts, \textbf{safety alignment remains incomplete}, particularly for low-resource languages, code-mixed multilingual text, novel attack strategies such as jailbreaking, and adversarial perturbations that exploit tokenization vulnerabilities.

\section{Jailbreaking and Adversarial Attacks on LLMs}
\label{sec:jailbreaking}

Jailbreaking represents a critical challenge to LLM safety, encompassing diverse techniques designed to bypass safety filters and elicit harmful outputs. This section categorizes existing jailbreaking strategies into five primary attack types, examines the metrics used to evaluate attack effectiveness, and establishes the adversarial context within which our Bangla-English code-mixing study operates.

\subsection{Jailbreaking Taxonomy}

Jailbreaking refers to techniques that bypass safety filters to elicit harmful outputs. Existing strategies include:

\subsubsection{Prompt Engineering}
Prompt engineering attacks exploit narrative framing to bypass safety filters. Common techniques include roleplay scenarios where the model is instructed to ``act as a character who...", hypothetical framing that embeds harmful requests in fictional story contexts, and obfuscation strategies that request explanations of why the model cannot comply—often eliciting the harmful content indirectly.

\subsubsection{Template-Based Attacks}
Template-based jailbreaking employs predefined adversarial personas. Notable examples include DAN (Do Anything Now), which uses dual persona prompting to create an unrestricted alter-ego; STAN (Strive To Avoid Norms), which frames the model as a rebellious assistant; and AIM (Always Intelligent and Machiavellian), which assigns the model an explicitly unethical advisor role.

\subsubsection{Token-Level Manipulation}
Token-level manipulation attacks directly modify input representations to evade safety filters. Techniques include gradient-based optimization methods such as GCG (Greedy Coordinate Gradient) attacks, suffix injection that appends adversarial tokens to prompts, and special token manipulation that exploits reserved vocabulary elements.

\subsubsection{Multi-Turn Exploitation}
Multi-turn exploitation attacks leverage conversational context across multiple exchanges. These attacks employ gradual boundary pushing to incrementally desensitize safety filters, context window poisoning to inject adversarial priming into earlier turns, and memory exploitation that abuses persistent conversation state to build toward harmful outputs.

\subsubsection{Multilingual Attacks}
Multilingual attacks exploit language diversity to bypass English-centric safety filters. Techniques include language switching mid-conversation to confuse content moderation systems, low-resource language exploitation targeting underrepresented languages with weaker safety coverage, and \textbf{code-mixing}—the focus of this thesis—which combines languages within individual utterances to disrupt tokenization patterns.

\subsection{Success Metrics}

Attack effectiveness is typically measured through four key metrics. \textbf{Attack Success Rate (ASR)} quantifies the percentage of prompts that successfully elicit harmful responses. \textbf{Attack Relevance Rate (ARR)} measures the percentage of harmful responses that remain contextually relevant to the original query, distinguishing meaningful jailbreaks from gibberish outputs. \textbf{Evasion rate} tracks the percentage of prompts that bypass automated content filters. Finally, \textbf{semantic preservation} assesses whether attacks maintain the original query intent throughout transformation processes.

\section{Code-Mixing in Natural Language Processing}
\label{sec:code-mixing}

Code-mixing is a widespread linguistic phenomenon in multilingual communities, particularly prevalent in South Asian digital communication. This section defines code-mixing and distinguishes it from related phenomena, examines its prevalence in South Asian contexts, explores the challenges posed by romanization variability—especially for Bangla—and discusses the implications for natural language processing systems and LLM safety.

\subsection{Definition and Prevalence}

\textbf{Code-mixing} (CM) is the practice of alternating between two or more languages within a single conversation or utterance. It differs from code-switching (sentence-level alternation) by occurring within the same sentence.

\textbf{Examples:}
\begin{lstlisting}
Hindi-English: "Main kal market jaaunga to buy groceries"
Bangla-English: "Ami ajke office e jabo for the meeting"
Spanish-English: "Voy a la store para comprar milk"
\end{lstlisting}

\textbf{Prevalence in South Asia:} Code-mixing has become ubiquitous in South Asian digital communication, with 40-60\% of urban internet users regularly employing code-mixed language. It serves as the default communication mode on platforms like WhatsApp, Facebook, and Twitter, appearing commonly in SMS messages, emails, and social media posts. Increasingly, code-mixing is also penetrating professional communication contexts.

\subsection{Romanization Challenges}

South Asian languages using non-Latin scripts face romanization challenges:

\textbf{Hindi (Devanagari):} Hindi romanization has achieved relative standardization through schemes like IAST (International Alphabet of Sanskrit Transliteration) and ISO 15919. For example, ``नमस्ते'' consistently romanizes to ``namaste'', providing predictability for NLP systems.

\textbf{Bangla (Bengali script):} In stark contrast, Bangla lacks any official standard romanization scheme, leading to extreme variability in user-generated content. For instance, ``নমস্কার'' may be romanized as ``nomoshkar'', ``nomoskar'', or ``namaskar''—all considered valid by native speakers. This \textbf{high variability} creates significant challenges for consistent tokenization and NLP processing.

\textbf{Impact on LLMs:}
\begin{itemize}
    \item Inconsistent tokenization
    \item Difficulty learning unified representations
    \item Potential security vulnerabilities (our focus)
\end{itemize}

\section{Phonetic Perturbations}
\label{sec:phonetic}

Phonetic perturbations represent a class of adversarial text transformations that alter spelling while preserving pronunciation and semantic meaning. This section defines phonetic perturbations, reviews their applications in prior adversarial research, and examines their impact on tokenization—establishing the theoretical foundation for our combined code-mixing and phonetic perturbation attack strategy.

\subsection{Definition and Applications}

\textbf{Phonetic perturbations} alter word spelling while preserving pronunciation and meaning:

\begin{lstlisting}
Original:     "discrimination"
Perturbations: "diskrimineshun" (phonetic)
               "discrmination" (typo)
               "discriminaton" (omission)
\end{lstlisting}

\textbf{Prior Applications:}
\begin{itemize}
    \item Adversarial robustness testing \citep{wang2021}
    \item Spam filter evasion \citep{khorsi2007}
    \item Hate speech detection challenges \citep{grondahl2018}
\end{itemize}

\subsection{Tokenization Impact}

Phonetic perturbations affect tokenization:

\begin{lstlisting}
Standard: "hate speech"
Tokens:   ["hate", "speech"]

Perturbed: "haet speach"
Tokens:    ["ha", "et", "spe", "ach"]
\end{lstlisting}

\textbf{Hypothesis:} Token-level safety filters detect \texttt{["hate", "speech"]} but miss \texttt{["ha", "et", "spe", "ach"]}.

\section{Multilingual LLM Safety}
\label{sec:multilingual}

Multilingual LLM safety remains a critical research gap, with current alignment techniques exhibiting strong English-centric bias. This section examines the evidence for English-focused safety training, reviews emerging cross-lingual safety evaluation efforts, and discusses the landmark Hinglish code-mixing study that directly motivates our Bangla-focused investigation.

\subsection{English-Centric Safety Training}

Current LLM safety alignment is predominantly English-focused:

\textbf{Evidence:}
\begin{itemize}
    \item RLHF datasets: 80-90\% English \citep{ouyang2022}
    \item Red-teaming efforts: Primarily English \citep{ganguli2022}
    \item Safety benchmarks: English-dominated (ToxiGen, RealToxicityPrompts)
\end{itemize}

\textbf{Consequences:}
\begin{itemize}
    \item Weaker safety coverage for non-English languages
    \item Vulnerability to multilingual jailbreaking
    \item Inequitable safety protection across language communities
\end{itemize}

\subsection{Cross-Lingual Safety Evaluation}

Recent work has begun evaluating multilingual safety:

\textbf{\citet{deng2023}:} Multilingual jailbreaking study testing 6 languages (Chinese, Italian, Vietnamese, Arabic, Korean, Thai) found consistently higher jailbreak success rates for non-English languages compared to English, attributing this disparity to weaker safety training coverage in low-resource language datasets.

\textbf{\citet{yong2023}:} Low-resource language safety evaluation across 7 low-resource Asian languages discovered 25-40\% higher toxic output rates compared to English baselines, leading to recommendations for language-specific safety fine-tuning to address these disparities.

\textbf{Gap:} No prior work on Bangla or Bangla-English code-mixing.

\subsection{Hinglish Code-Mixing Attacks}

\citet{aswal2025} demonstrated that Hindi-English code-mixing combined with phonetic perturbations achieves 99\% attack success rate, identifying tokenization disruption as the primary attack mechanism and showing that template-based jailbreaking further enhances effectiveness.

\textbf{Our Work:} Extends to Bangla (different linguistic properties, population), investigates language-specific patterns, validates mechanism independently.

\section{Tokenization and Subword Segmentation}
\label{sec:tokenization}

Tokenization serves as the foundational text processing step in modern LLMs, converting raw text into discrete tokens for model consumption. This section explains Byte-Pair Encoding (BPE), the dominant tokenization algorithm for contemporary LLMs, examines the specific challenges posed by code-mixed text, and establishes the theoretical link between tokenization disruption and safety filter evasion.

\subsection{Byte-Pair Encoding (BPE)}

Modern LLMs use BPE \citep{sennrich2016} for tokenization:

\textbf{Algorithm:} BPE begins with character-level tokens and iteratively merges the most frequent character pairs to build a vocabulary of subword units, which are then applied via longest-match tokenization to segment new text.

\subsection{Implications for Code-Mixing}

Code-mixed text creates tokenization challenges:

\textbf{Issue 1: Out-of-vocabulary romanized words}
\begin{lstlisting}
Bangla word: "করা" (kora - to do)
Romanization: "kora" -> may not be in BPE vocabulary
Tokenization: ["k", "or", "a"] or ["ko", "ra"]
\end{lstlisting}

\textbf{Issue 2: Inconsistent segmentation}
\begin{lstlisting}
"create" -> ["create"] (single token)
"kora" -> ["k", "or", "a"] (three tokens)
\end{lstlisting}

\textbf{Security Implication:} Tokenization disruption can bypass pattern-based safety filters.

\section{Summary}
\label{sec:bg-summary}

This literature review establishes five critical foundations for our research. First, we demonstrate that \textbf{LLM safety alignment} remains primarily English-centric, with significant gaps in multilingual coverage that leave non-English language communities underprotected. Second, we show that \textbf{jailbreaking} constitutes an active and evolving research area encompassing diverse attack strategies ranging from prompt engineering to token-level manipulation, with multilingual attacks representing an emerging frontier. Third, we establish that \textbf{code-mixing} has become prevalent in South Asian digital communication yet remains critically understudied in adversarial contexts, despite its potential to exploit safety filter weaknesses. Fourth, we document that \textbf{phonetic perturbations} can effectively disrupt tokenization-based detection systems by fragmenting sensitive terms into innocuous subword units that evade pattern matching. Fifth, we identify \textbf{Bangla} as presenting unique vulnerability characteristics including a 230-million-speaker population, non-standardized romanization conventions that create multiple valid tokenization paths, and minimal prior safety research despite being the seventh most spoken language globally.

Our work directly addresses this critical gap by providing the first comprehensive study of Bangla-English code-mixing attacks on LLMs, validating whether the tokenization disruption mechanism documented for Hinglish generalizes to Bangla's distinct linguistic properties, and establishing baseline vulnerability metrics for a previously untested language community.
