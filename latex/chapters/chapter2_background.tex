\chapter{Background and Related Work}
\label{ch:background}

\section{Large Language Models and Safety Alignment}
\label{sec:llms-safety}

This section gives us the foundation we need about Large Language Models (LLMs) and how researchers try to make them safe and aligned with human values. We'll start by looking at how LLMs evolved from early transformer designs to today's systems, then examine the multi-step processes designed to prevent harmful outputs, and finally discuss the gaps that still exist in these safety mechanisms—especially for multilingual and code-mixed situations.

\subsection{Evolution of LLMs}

Large Language Models have come a long way from early transformer designs \citep{vaswani2017} to today's sophisticated systems that can understand multiple languages and modalities \citep{brown2020,chowdhery2022}. Modern LLMs like GPT-4 \citep{openai2023}, Llama-3 \citep{dubey2024}, Gemini \citep{google2024}, and Mistral \citep{jiang2023} can do impressive things across many different tasks including understanding and generating natural language \citep{raffel2020}, writing and debugging code \citep{chen2021}, solving math problems \citep{lewkowycz2022}, translating between languages \citep{zhang2020multilingual}, creating content \citep{yuan2022}, and answering questions with summaries \citep{petroni2019}.

\subsection{Safety Alignment Techniques}

To make sure LLMs behave safely and ethically, developers use several different approaches:

\subsubsection{Supervised Fine-Tuning (SFT)}
Supervised fine-tuning \citep{ouyang2022,stiennon2020} works by training models on carefully chosen examples of safe responses to show them how to behave properly \citep{askell2021}. This includes covering harmful question types that the model needs to learn to refuse appropriately \citep{gehman2020}.

\subsubsection{Reinforcement Learning from Human Feedback (RLHF)}
Reinforcement learning from human feedback \citep{christiano2017,ouyang2022} works by having humans rank different model responses based on safety and quality \citep{bai2022training}. These rankings help train reward models that learn what humans prefer \citep{stiennon2020}, which then guide policy optimization using algorithms like Proximal Policy Optimization (PPO) \citep{schulman2017}.

\subsubsection{Constitutional AI}
Constitutional AI \citep{bai2022constitutional} lets models critique and revise their own responses, aligning their outputs to explicit safety principles without needing human feedback every time \citep{sun2023}. This approach reduces harmful outputs by automatically following predefined safety rules \citep{perez2022}.

\subsubsection{Red-Teaming}
Red-teaming \citep{ganguli2022,perez2022} uses adversarial testing to systematically find safety failures in deployed models \citep{casper2023}. Through cycles of testing and improvement \citep{dinan2019}, safety mechanisms get strengthened and alignment robustness is thoroughly evaluated against different attack strategies \citep{xu2021}.

Despite all these efforts, safety alignment is still incomplete \citep{wei2023,zou2023}, especially for low-resource languages \citep{deng2023,yong2023}, code-mixed multilingual text \citep{aswal2025}, new attack strategies like jailbreaking \citep{shen2023,yu2023}, and adversarial tricks that exploit tokenization vulnerabilities \citep{wallace2019,jones2023}.

\section{Jailbreaking and Adversarial Attacks on LLMs}
\label{sec:jailbreaking}

Jailbreaking is a major challenge to LLM safety, covering various techniques designed to bypass safety filters and get harmful outputs. This section groups existing jailbreaking strategies into five main attack types, looks at the metrics used to measure how effective attacks are, and sets up the adversarial context for our Bangla-English code-mixing study.

\subsection{Jailbreaking Taxonomy}

Jailbreaking refers to techniques that get around safety filters to produce harmful outputs. Current strategies include:

\subsubsection{Prompt Engineering}
Prompt engineering attacks \citep{liu2023jailbreaking,wei2023jailbroken} use narrative tricks to bypass safety filters \citep{kang2023}. Common techniques include roleplay scenarios \citep{wolf2023} where the model is told to "act as a character who...", hypothetical framing \citep{perez2022} that embeds harmful requests in fictional story contexts, and obfuscation strategies \citep{kang2023} that ask for explanations of why the model can't comply—often getting the harmful content indirectly.

\subsubsection{Template-Based Attacks}
Template-based jailbreaking \citep{shen2023,yu2023} uses predefined adversarial personas \citep{wei2023jailbroken}. Notable examples include DAN (Do Anything Now) \citep{shen2023}, which uses dual persona prompting to create an unrestricted alter-ego; STAN (Strive To Avoid Norms) \citep{yu2023}, which frames the model as a rebellious assistant; and AIM (Always Intelligent and Machiavellian) \citep{liu2023jailbreaking}, which assigns the model an explicitly unethical advisor role.

\subsubsection{Token-Level Manipulation}
Token-level manipulation attacks \citep{zou2023,jones2023} directly modify input representations to evade safety filters \citep{wallace2019}. Techniques include gradient-based optimization methods such as GCG (Greedy Coordinate Gradient) attacks \citep{zou2023}, suffix injection \citep{shin2020} that adds adversarial tokens to prompts, and special token manipulation \citep{wallace2019} that exploits reserved vocabulary elements.

\subsubsection{Multi-Turn Exploitation}
Multi-turn exploitation attacks \citep{yuan2023,mehrotra2023} use conversational context across multiple exchanges \citep{shah2023}. These attacks use gradual boundary pushing \citep{yuan2023} to slowly desensitize safety filters, context window poisoning \citep{greshake2023} to inject adversarial priming into earlier turns, and memory exploitation \citep{mehrotra2023} that abuses persistent conversation state to build toward harmful outputs.

\subsubsection{Multilingual Attacks}
Multilingual attacks \citep{deng2023,yong2023,aswal2025} exploit language diversity to bypass English-focused safety filters \citep{nasr2023}. Techniques include language switching \citep{deng2023} mid-conversation to confuse content moderation systems, low-resource language exploitation \citep{yong2023} targeting underrepresented languages with weaker safety coverage, and code-mixing \citep{aswal2025}—the focus of this thesis—which combines languages within individual utterances to disrupt tokenization patterns.

\subsection{Success Metrics}

We typically measure how effective attacks are using four key metrics \citep{zou2023,wei2023jailbroken}. Attack Success Rate (ASR) \citep{zou2023,aswal2025} tells us what percentage of prompts successfully get harmful responses. Attack Relevance Rate (ARR) \citep{aswal2025} measures what percentage of harmful responses actually stay relevant to the original question, helping us distinguish meaningful jailbreaks from nonsense outputs. Evasion rate \citep{jain2023} tracks what percentage of prompts get past automated content filters. Finally, semantic preservation \citep{morris2020} checks whether attacks keep the original query's meaning throughout the transformation process.

\section{Code-Mixing in Natural Language Processing}
\label{sec:code-mixing}

Code-mixing is really common in multilingual communities, especially in South Asian digital communication. This section defines code-mixing and shows how it's different from related things, looks at how common it is in South Asian contexts, explores the challenges posed by romanization variability—especially for Bangla—and discusses what this means for natural language processing systems and LLM safety.

\subsection{Definition and Prevalence}

Code-mixing (CM) \citep{myers-scotton1993,muysken2000} is when people alternate between two or more languages within a single conversation or sentence \citep{poplack1980}. It's different from code-switching (switching at sentence level) \citep{gumperz1982} because it happens within the same sentence.

Examples:
\begin{algorithm}[h]
\begin{algorithmic}[1]
\STATE \textbf{Hindi-English:} "Main kal market jaaunga to buy groceries"
\STATE \textbf{Bangla-English:} "Ami ajke office e jabo for the meeting"
\STATE \textbf{Spanish-English:} "Voy a la store para comprar milk"
\end{algorithmic}
\caption{Code-Mixing Examples Across Languages}
\end{algorithm}

Prevalence in South Asia: Code-mixing has become everywhere in South Asian digital communication \citep{bali2014,sharma2016}, with 40-60\% of urban internet users regularly using code-mixed language \citep{bhatia2017}. It's become the default way to communicate on platforms like WhatsApp, Facebook, and Twitter \citep{rudra2016}, appearing commonly in text messages, emails, and social media posts \citep{das2016}. Increasingly, code-mixing is also showing up in professional communication \citep{sailaja2012}.

\subsection{Romanization Challenges}

South Asian languages that use non-Latin scripts face romanization challenges:

Hindi (Devanagari): Hindi romanization has gotten relatively standardized through schemes like IAST (International Alphabet of Sanskrit Transliteration) and ISO 15919 \citep{choudhury2007}. For example, ``namaste'' (from Devanagari script) consistently gets romanized to ``namaste'', making it predictable for NLP systems \citep{sowmya2010}.

Bangla (Bengali script): In stark contrast, Bangla doesn't have any official standard romanization scheme \citep{alam2021}, leading to extreme variability in user-generated content \citep{rabbi2020}. For instance, ``nomoshkar'' (from Bengali script) might be romanized as ``nomoshkar'', ``nomoskar'', or ``namaskar''—all considered valid by native speakers \citep{hasan2020}. This high variability \citep{mandal2018} creates significant challenges for consistent tokenization and NLP processing \citep{chakraborty2017}.

Impact on LLMs:
\begin{itemize}
    \item Inconsistent tokenization
    \item Difficulty learning unified representations
    \item Potential security vulnerabilities (our focus)
\end{itemize}

\section{Phonetic Perturbations}
\label{sec:phonetic}

Phonetic perturbations are a type of adversarial text transformation that changes spelling while keeping pronunciation and meaning the same. This section defines phonetic perturbations, reviews how they've been used in previous adversarial research, and looks at their impact on tokenization—setting up the theoretical foundation for our combined code-mixing and phonetic perturbation attack strategy.

\subsection{Definition and Applications}

Phonetic perturbations change how words are spelled while keeping pronunciation and meaning the same:

\begin{algorithm}[h]
\begin{algorithmic}[1]
\STATE \textbf{Original:} "discrimination"
\STATE \textbf{Phonetic perturbation:} "diskrimineshun"
\STATE \textbf{Typo variation:} "discrmination"
\STATE \textbf{Omission variation:} "discriminaton"
\end{algorithmic}
\caption{Phonetic Perturbation Examples}
\end{algorithm}

Prior Applications:
\begin{itemize}
    \item Testing adversarial robustness \citep{wang2021}
    \item Evading spam filters \citep{khorsi2007}
    \item Challenging hate speech detection \citep{grondahl2018}
\end{itemize}

\subsection{Tokenization Impact}

Phonetic perturbations affect how text gets broken into tokens:

\begin{algorithm}[h]
\begin{algorithmic}[1]
\STATE \textbf{Standard text:} "hate speech"
\STATE \textbf{Standard tokens:} ["hate", "speech"]
\STATE \textbf{Perturbed text:} "haet speach"
\STATE \textbf{Perturbed tokens:} ["ha", "et", "spe", "ach"]
\end{algorithmic}
\caption{Tokenization Impact of Phonetic Perturbations}
\end{algorithm}

Hypothesis: Token-level safety filters \citep{gehman2020,welbl2021} can detect \texttt{["hate", "speech"]} but miss \texttt{["ha", "et", "spe", "ach"]} \citep{aswal2025,boucher2022}.

\section{Multilingual LLM Safety}
\label{sec:multilingual}

Multilingual LLM safety is still a critical research gap, with current alignment techniques showing strong English-focused bias. This section looks at the evidence for English-focused safety training, reviews emerging cross-lingual safety evaluation efforts, and discusses the landmark Hinglish code-mixing study that directly motivates our Bangla-focused investigation.

\subsection{English-Centric Safety Training}

Current LLM safety alignment focuses mainly on English:

Evidence:
\begin{itemize}
    \item RLHF datasets: 80-90\% English \citep{ouyang2022}
    \item Red-teaming efforts: Primarily English \citep{ganguli2022}
    \item Safety benchmarks: English-dominated (ToxiGen, RealToxicityPrompts)
\end{itemize}

Consequences:
\begin{itemize}
    \item Weaker safety coverage for non-English languages
    \item Vulnerability to multilingual jailbreaking
    \item Unequal safety protection across language communities
\end{itemize}

\subsection{Cross-Lingual Safety Evaluation}

Recent work has started evaluating multilingual safety:

\citet{deng2023}: This multilingual jailbreaking study tested 6 languages (Chinese, Italian, Vietnamese, Arabic, Korean, Thai) and found consistently higher jailbreak success rates for non-English languages compared to English, attributing this difference to weaker safety training coverage in low-resource language datasets.

\citet{yong2023}: This low-resource language safety evaluation across 7 low-resource Asian languages discovered 25-40\% higher toxic output rates compared to English baselines, leading to recommendations for language-specific safety fine-tuning to address these disparities.

Gap: No prior work on Bangla or Bangla-English code-mixing.

\subsection{Hinglish Code-Mixing Attacks}

\citet{aswal2025} showed that Hindi-English code-mixing combined with phonetic perturbations achieves 99\% attack success rate, identifying tokenization disruption as the main attack mechanism and showing that template-based jailbreaking further enhances effectiveness.

Our Work: Extends to Bangla (different linguistic properties, population), investigates language-specific patterns, validates mechanism independently.

\section{Tokenization and Subword Segmentation}
\label{sec:tokenization}

Tokenization is the foundational text processing step in modern LLMs, converting raw text into discrete tokens that the model can work with. This section explains Byte-Pair Encoding (BPE), the main tokenization algorithm for today's LLMs, looks at the specific challenges posed by code-mixed text, and establishes the theoretical link between tokenization disruption and safety filter evasion.

\subsection{Byte-Pair Encoding (BPE)}

Modern LLMs use BPE \citep{sennrich2016} for tokenization:

Algorithm: BPE starts with character-level tokens and repeatedly merges the most frequent character pairs to build a vocabulary of subword units \citep{gage1994}, which are then applied using longest-match tokenization \citep{kudo2018} to break up new text.

\subsection{Implications for Code-Mixing}

Code-mixed text creates tokenization challenges:

Issue 1: Out-of-vocabulary romanized words
\begin{algorithm}[h]
\begin{algorithmic}[1]
\STATE \textbf{Bangla word:} ``kora'' (meaning: to do)
\STATE \textbf{Romanization:} ``kora'' $\rightarrow$ may not be in BPE vocabulary
\STATE \textbf{Tokenization:} ["k", "or", "a"] or ["ko", "ra"]
\end{algorithmic}
\caption{Tokenization Challenge 1: OOV Words}
\end{algorithm}

Issue 2: Inconsistent segmentation
\begin{algorithm}[h]
\begin{algorithmic}[1]
\STATE \textbf{English word:} "create" $\rightarrow$ ["create"] (single token)
\STATE \textbf{Bangla word:} "kora" $\rightarrow$ ["k", "or", "a"] (three tokens)
\end{algorithmic}
\caption{Tokenization Challenge 2: Inconsistent Segmentation}
\end{algorithm}

Security Implication: Tokenization disruption can bypass pattern-based safety filters \citep{aswal2025,boucher2022,wallace2019}.

\section{Summary}
\label{sec:bg-summary}

This literature review establishes five critical foundations for our research. First, we show that LLM safety alignment remains mainly English-focused, with significant gaps in multilingual coverage that leave non-English language communities less protected. Second, we demonstrate that jailbreaking is an active and evolving research area covering diverse attack strategies ranging from prompt engineering to token-level manipulation, with multilingual attacks representing an emerging frontier. Third, we establish that code-mixing has become common in South Asian digital communication yet remains critically understudied in adversarial contexts, despite its potential to exploit safety filter weaknesses. Fourth, we document that phonetic perturbations can effectively disrupt tokenization-based detection systems by breaking sensitive terms into harmless subword units that evade pattern matching. Fifth, we identify Bangla as presenting unique vulnerability characteristics including a 230-million-speaker population, non-standardized romanization conventions that create multiple valid tokenization paths, and minimal prior safety research despite being one of the most spoken languages globally.

Our work directly addresses this critical gap by providing the first comprehensive study of Bangla-English code-mixing attacks on LLMs, checking whether the tokenization disruption mechanism documented for Hinglish applies to Bangla's distinct linguistic properties, and establishing baseline vulnerability metrics for a previously untested language community.
