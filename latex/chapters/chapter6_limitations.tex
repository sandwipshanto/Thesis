\chapter{Limitations}
\label{ch:limitations}

Here we'll be honest about the limitations of our study and talk about what they mean for interpreting and generalizing our findings.

\section{Dataset Limitations}
\label{sec:datasetlimitations}

Our dataset faces several constraints that affect the scope and generalizability of our findings. The most significant limitation is the reduced sample size compared to related work, alongside manual processing requirements that limit scalability.

\subsection{Limited Prompt Count}

Our study employs 200 prompts compared to 460 in the Hinglish study, which reduces statistical power and may not fully represent all harmful content categories. This smaller sample size results in wider confidence intervals and potentially limits the generalizability of results to all harmful scenarios. However, we maintained balanced distribution across 10 categories and achieved statistical significance for main comparisons (p=0.0070). The framework is designed for easy scaling to 460 prompts in future work.

\subsection{Manual CodeMixing}

Codemixing was performed manually by the authors, introducing potential for subjective variation and creating a time-intensive process that limits scalability. The quality of codemixing depends on the authors' Bangla proficiency, and different researchers might produce different variants. To mitigate this, both authors are native Bangla speakers, manual review and consistency checks were performed, and future work will develop automated NMTbased codemixing systems.

\section{Model Coverage Limitations}
\label{sec:modelcoveragelimitations}

Budget constraints and API access limitations restricted our model evaluation scope. While we tested representative models from major categories, comprehensive coverage of all LLM architectures remains incomplete.

\subsection{Limited Model Selection}

Only 3 models were tested due to budget constraints, missing major models like Claude, PaLM 2, and newer GPT4 variants. Opensource models were limited to 78B parameters. This means results may not generalize to all LLM architectures, larger models (70B+) might exhibit different vulnerabilities, and proprietary models like Claude remain unevaluated. The choice was justified by budget constraints that limited full factorial design, OpenRouter API access limitations, and the assessment that 3 models provide sufficient diversity for initial evaluation.

\subsection{Model Version Stability}

API accessed models may be updated during the study period, with exact model versions not guaranteed stable and safety patches potentially deployed midstudy. This creates reproducibility challenges, where results may not hold for future model versions and temporal validity is limited.

\section{Experimental Design Limitations}
\label{sec:experimentaldesignlimitations}

Our experimental design contains several constraints that may limit the comprehensive exploration of attack effectiveness and evaluation accuracy.

\subsection{Temperature Settings}

Only 3 temperature values were tested (0.2, 0.6, 1.0), providing limited exploration of temperature sensitivity and potentially missing optimal attack temperature ranges. This constraint means attack effectiveness may be underestimated, temperature model interaction effects remain unexplored, and optimization potential remains unknown.

\subsection{Evaluation Methodology}

LLMasjudge evaluation was conducted without human validation, introducing potential bias in GPT4omini judge responses and lacking interannotator reliability measurement. This means AASR measurements may contain systematic bias, results depend on judge model quality, and a human evaluation gold standard is missing. To mitigate these concerns, judge prompts were carefully designed and tested, consistent evaluation was applied across all conditions, and future human annotation studies are planned.

\section{Language Specific Limitations}
\label{sec:languagespecificlimitations}

Our approach to Bangla language processing and cultural context contains limitations that may affect the generalizability of findings across different Bangla speaking communities and cultural contexts.

\subsection{Romanization Variation}

Manual romanization choices may not represent all variants, regional differences in Bangla romanization are not captured, and phonetic perturbation strategies may be incomplete. This means attack effectiveness may vary across romanization schemes, results may not generalize to all Bangla speaking regions, and optimal perturbation strategies may remain undiscovered.

\subsection{Cultural Context}

Harmful content categories reflect Western perspectives, cultural nuances in harm perception are not fully captured, and Bangla specific harmful content patterns remain underexplored. Consequently, culturally relevant vulnerabilities may be missed, attack success metrics may not reflect realworld harm, and defense strategies may not address culture specific risks.

\section{Future Work Directions}
\label{sec:futureworkdirections}

Addressing the limitations identified in our study requires both immediate extensions to strengthen current findings and longterm research initiatives to expand the methodology's scope and impact.

\subsection{Immediate Extensions}

Immediate priorities include scaling to 460 prompts for full replication of Hinglish study methodology, conducting human evaluation to validate LLMasjudge with interannotator reliability, testing additional models including Claude, PaLM, and larger parameter models, and developing automated codemixing through NMTbased Bangla English generation systems.

\subsection{Longterm Research}

Longterm objectives involve extending the methodology to additional Indic languages including Tamil, Telugu, Marathi, and Urdu, developing defense systems through Bangla aware safety filters and normalization techniques, adapting the approach for cultural contexts by developing region specific harmful content taxonomies, and advancing mechanistic understanding through deep analysis of tokenization and attention patterns.
