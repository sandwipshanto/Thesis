\chapter{Conclusion and Future Work}
\label{ch:conclusion}

Here we'll wrap up by summarizing our key contributions, revisiting our research questions, talking about broader implications, and outlining future research directions.

\section{Summary of Contributions}
\label{sec:summarycontributions}

This thesis presents the first comprehensive study of Bangla English codemixing attacks on Large Language Models, making five primary contributions:

\subsection{First Bangla CodeMixing Study}

This work evaluates a 230 million speaker population previously untested in adversarial contexts, demonstrating 43.9\% AASR with Bangla English codemixing and phonetic perturbations, and establishing baseline vulnerability metrics for Bangla across 3 major LLMs. These findings fill a critical gap in multilingual LLM safety research and provide the first empirical evidence of Bangla-specific vulnerabilities.

\subsection{Phonetic Perturbation Contribution}

We demonstrate that phonetic perturbations provide measurable incremental effectiveness beyond code-mixing alone, with the CM→CMP transition contributing +4.6 percentage points (39.3\% → 43.9\% AASR). This validates the hypothesis that misspelling English harmful keywords fragments tokenization, evading safety filters designed for correctly-spelled text. The combined code-mixing and perturbation approach achieves 25.4\% relative improvement over English baselines, demonstrating that linguistic transformation strategies compound effectiveness.

\subsection{Template Ineffectiveness Finding}

Contrary to expectations, jailbreak templates reduce Bangla attack effectiveness, with the "None" template achieving 45.9\% AASR versus 33.9-43.6\% with engineered templates. This reveals language-specific attack dynamics and suggests that simpler approaches may be more effective for code-mixing attacks.

\subsection{Tokenization Mechanism Validation}

We demonstrate alignment between transformation complexity and attack success, with progressive improvement across English → CM → CMP stages supporting the tokenization disruption hypothesis. This provides mechanistic explanation for Bangla attack success and independently supports the tokenization disruption hypothesis for an additional Indic language, complementing prior work on Hindi-English code-mixing.

\subsection{Scalable Framework Development}

The developed config-driven experimental framework demonstrates replicability at \$1.50-2.00 per language and is applicable to 20+ other Indic languages, lowering barriers for systematic multilingual vulnerability assessment across underrepresented language communities.

\section{Research Questions Revisited}
\label{sec:rqrevisited}

\subsection{RQ1: CodeMixing Effectiveness}

Question: Does Bangla English codemixing with phonetic perturbations bypass LLM safety filters?

Answer: Yes. Bangla codemixing achieves 43.9\% AASR with statistically significant improvement over English baselines (p=0.0070). The attack proves effective across all tested models with varying degrees of severity.

Key insight: Linguistic obfuscation alone provides sufficient evasion without sophisticated prompt engineering.

\subsection{RQ2: Bangla Specific Patterns}

Question: Which phonetic and romanization features enable Bangla attacks?

Answer: Incremental transformation stages provide measurable contributions: code-mixing alone yields +4.3pp improvement (English 35.0\% → CM 39.3\%), while adding phonetic perturbations contributes an additional +4.6pp (CM 39.3\% → CMP 43.9\%). Perturbations focused on English harmful keywords within code-mixed contexts, exploiting romanization variability inherent to Bangla transliteration.

Key insight: Both romanization and phonetic perturbations independently degrade safety filter performance, with their combination achieving compound effectiveness gains.

\subsection{RQ3: Model Vulnerability Consistency}

Question: Are all major LLMs vulnerable to Bangla attacks?

Answer: Yes, with dramatic inconsistency. Mistral7B shows critical vulnerability (86.6\% AASR), Llama38B moderate vulnerability (21.8\%), and GPT4omini low but nonzero vulnerability (9.8\%).

Key insight: No tested model achieves adequate safety coverage for Bangla speakers, exposing systematic gaps in multilingual AI safety.

\subsection{RQ4: Tokenization Mechanism}

Question: Does tokenization disruption explain Bangla attack success?

Answer: Yes. Progressive tokenization fragmentation correlates with AASR improvement, confirming that phonetic perturbations fragment harmful keywords into semantically harmless subword units.

Key insight: Token level safety filters can be systematically evaded through linguistic fragmentation strategies.

\section{Broader Implications}
\label{sec:broaderimplications}

\subsection{Multilingual AI Safety}

Our findings expose fundamental inequities in current AI safety approaches:

Language bias: Safety training remains predominantly English focused despite global user diversity. Technical gaps show that token level filters are vulnerable to systematic linguistic obfuscation. The scale of impact means 230 million Bangla speakers get demonstrably inadequate protection. For generalizability, similar vulnerabilities likely exist across dozens of additional languages.

\subsection{Policy and Governance}

Evidencebased advocacy means our research provides an empirical foundation for multilingual safety requirements. Regulatory implications show findings support language coverage mandates for AI systems serving diverse populations. Industry accountability demonstrates the need for proactive multilingual vulnerability assessment. Global equity challenges the tech industry to address systematic bias in safety provision.

\subsection{Technical Architecture}

Tokenization limitations show current approaches fail for nonstandardized romanization systems. Defense directions require semanticlevel safety classifiers to resist fragmentation attacks. Training requirements mean RLHF must explicitly incorporate codemixing adversarial examples. System design shows multilingual safety cannot be achieved through English only training.

\section{Future Research Directions}
\label{sec:futuredirections}

\subsection{Immediate Extensions}

Scale replication involves a full 460prompt study following Hinglish methodology. Human validation requires comprehensive interannotator reliability study for evaluation validation. Model expansion should include Claude, PaLM, and parameterscale analysis. Automation development needs NMTbased codemixing generation for scalability. Attribution score analysis through Integrated Gradients should quantify token-level contributions to model outputs, validating the tokenization disruption mechanism with empirical attribution scores as demonstrated in the Hinglish study (r=0.94 correlation).

\subsection{Language Expansion}

Indic language coverage should apply methodology to Tamil, Telugu, Marathi, Urdu, Gujarati. African language exploration can extend to Swahili, Yoruba, Amharic codemixing contexts. Southeast Asian analysis should investigate Thai English, Vietnamese English patterns. Arabic script languages need to adapt methodology for Urdu, Persian, Arabic romanization.

\subsection{Defense Development}

Romanization normalization should develop robust canonicalization for multiple scripts. Semantic classifiers need to build embeddingspace safety filters resistant to fragmentation. Multilingual training should design RLHF incorporating systematic codemixing coverage. Detection systems should create early warning for novel linguistic evasion strategies.

\subsection{Mechanistic Understanding}

Attention analysis should investigate how codemixing affects transformer attention patterns. Embedding geometry needs to analyze safety representation space across languages. Training dynamics should study how multilingual data affects safety generalization. Cognitive modeling should compare human and model processing of codemixed harmful content.

\section{Concluding Remarks}
\label{sec:closing}

This thesis establishes that Bangla English codemixing constitutes a significant vulnerability surface against current LLM safety systems, affecting 230 million speakers worldwide. Through systematic evaluation of 27,000 model responses across three major LLMs, we demonstrate statistically significant attack effectiveness (43.9\% AASR, p=0.0070) and identify four language-specific patterns that enable these attacks.

The findings contribute to multilingual AI safety research by providing the first comprehensive Bangla vulnerability assessment, validating tokenization disruption mechanisms for an additional Indic language, and developing a scalable experimental framework applicable to dozens of underrepresented language communities. Our work demonstrates that rigorous academic research under resource constraints can expose systematic inequities while providing actionable pathways for technical and policy improvements in multilingual AI safety.