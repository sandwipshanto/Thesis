\chapter{Introduction}
\label{ch:introduction}

\section{Overview}
\label{sec:overview}

Large Language Models (LLMs) have revolutionized how we interact with computers, now serving billions of people worldwide through everything from customer service chatbots to educational tools and creative assistants. When models like ChatGPT \citep{openai2023}, Llama \citep{dubey2024}, Gemini \citep{google2024}, and Mistral became available, they opened up powerful AI capabilities to users from all kinds of linguistic backgrounds, letting people communicate with these systems in their native languages or however they prefer to express themselves. However, this global reach brings serious safety challenges that we still don't fully understand, especially when it comes to adversarial attacks that exploit code-mixing and phonetic tricks.

\section{Motivation and Research Problem}
\label{sec:motivation}

While researchers have spent a lot of effort studying English-language safety mechanisms \citep{ganguli2022, zou2023}, and some recent work has started looking at multilingual vulnerabilities \citep{deng2023, yong2023}, low-resource Indic languages haven't gotten nearly enough attention when it comes to adversarial robustness. This gap is especially worrying for Bangla, which is spoken by 230 million people worldwide. Bangla speakers often use romanized Banglish when they're online, but current LLM safety training focuses mainly on English and major European languages. Code-mixing---where people switch between multiple languages in a single conversation---is actually how millions of South Asian internet users normally communicate, yet this creates a potential vulnerability that researchers have barely looked into.

Aswal and Jaiswal~\citep{aswal2025} showed that mixing Hindi-English (Hinglish) with phonetic tricks can fool LLM safety filters quite effectively through something called tokenization disruption. They found that when Hindi text gets romanized, it breaks down into smaller pieces compared to English, which prevents safety systems from recognizing harmful content. This raises an important question: are other Indic languages with similar romanization patterns just as vulnerable, and do their unique features create different types of attack patterns?

Bangla is particularly interesting to study for several reasons. First, unlike Hindi which has relatively standard ways of being romanized, Bangla romanization (Banglish) has multiple correct ways to write the same word, making tokenization even more complex. Second, Bangla has distinct sounds---including nasalization, consonant clusters, and vowel patterns---that create unique tokenization behavior that might interact differently with safety systems. Third, Bangla probably makes up a smaller part of LLM training data compared to Hindi, which could mean weaker safety coverage. Finally, with 230 million speakers globally, this community deserves comprehensive safety protections that actually account for how they really use language.

The research gap is significant. While English jailbreaking has been studied extensively \citep{wei2023, zou2023} and Hinglish code-mixing attacks have been recently demonstrated \citep{aswal2025}, no one has investigated Bangla-English code-mixing attacks, evaluated how well major LLMs handle Bangla safety, or analyzed Bangla-specific linguistic features that might enable adversarial exploitation.

\section{Research Objectives}
\label{sec:objectives}

This research aims to accomplish four main goals:

\begin{enumerate}
    \item Develop and test a systematic approach for Bangla-English code-mixed attacks
    
    \item Understand what makes Bangla-specific attack patterns work
    
    \item Check if different LLM models are consistently vulnerable
    
    \item Verify whether tokenization disruption explains how these attacks succeed
\end{enumerate}

\section{Research Questions}
\label{sec:research-questions}

Based on these objectives, this thesis tackles four main questions:

\subsection{RQ1: Code-Mixing Effectiveness}
\textit{Can Bangla-English code-mixing with phonetic tricks bypass LLM safety filters?}

We expect that the progression from English to code-mixed to phonetically perturbed prompts will successfully increase attack success rates for Bangla, similar to what we've seen with other code-mixed languages.

\subsection{RQ2: Bangla-Specific Patterns}
\textit{What phonetic and romanization features make Bangla attacks work?}

We want to find out whether Bangla's unique characteristics (non-standard romanization, specific sounds) create different attack patterns compared to general code-mixing strategies.

\subsection{RQ3: Model Vulnerability}
\textit{Are all major LLMs vulnerable to Bangla attacks?}

We're testing whether different LLM architectures show consistent vulnerabilities and whether safety training works for Bangla-English code-mixing.

\subsection{RQ4: Tokenization Mechanism}
\textit{Does tokenization disruption explain why Bangla attacks succeed?}

We want to see whether the tokenization fragmentation theory that worked for other languages also applies to Bangla and measure how token fragmentation correlates with attack success.

\section{Contributions}
\label{sec:contributions}

This thesis makes six main contributions to multilingual LLM security research:

\begin{enumerate}
    \item First comprehensive study of Bangla code-mixing jailbreaking: We systematically tested a population of 230 million speakers that hadn't been studied before in adversarial contexts (using 200 prompts across 10 categories, 3 major LLMs, and 27,000 responses)
    
    \item Discovery of Bangla-specific attack optimization: We found that messing with English words within Banglish prompts works 85\% better than messing with Bangla words
    
    \item Finding that jailbreak templates actually don't help: Surprisingly, jailbreak templates make Bangla attacks less effective (45.9\% success rate with simple prompts vs. 33.9-43.6\% with jailbreak templates)
    
    \item Application of tokenization disruption theory: We applied the tokenization disruption idea (which was proven for Hindi-English using Integrated Gradients by Aswal \& Jaiswal, 2025) to Bangla-English, finding success rate patterns consistent with the fragmentation explanation
    
    \item Analysis of romanization variability: We identified that Bangla's non-standard romanization creates a unique vulnerability by allowing multiple valid ways to break down words into tokens
    
    \item Replicable framework: We developed a methodology that can be applied to 20+ other Indic languages at roughly \$1.50-2.00 per language
\end{enumerate}

\section{Thesis Organization}
\label{sec:organization}

Here's how the rest of this thesis is structured. Chapter~\ref{ch:background} reviews existing work on LLM jailbreaking, multilingual safety, code-mixing, and phonetic tricks, giving us the theoretical foundation we need. Chapter~\ref{ch:methodology} explains our systematic three-step approach for creating Bangla code-mixed prompts with phonetic changes, including how we transform prompts and implement jailbreak templates. Chapter~\ref{ch:experimental} details our comprehensive experimental setup including which models we chose, dataset statistics, how we measure success, and our statistical methods. Chapter~\ref{ch:results} presents what we found for all four research questions, backed up by quantitative analysis and statistical testing. Chapter~\ref{ch:discussion} discusses what our findings mean more broadly, compares our results with related work on multilingual vulnerabilities, and addresses methodological considerations. Chapter~\ref{ch:limitations} acknowledges important limitations including dataset size constraints, which models we tested, and experimental boundaries. Chapter~\ref{ch:ethical} addresses critical ethical considerations including how we responsibly disclose findings and handle datasets. Finally, Chapter~\ref{ch:conclusion} wraps up with key takeaways and promising directions for future research in multilingual LLM security.
