\chapter{Results}
\label{ch:results}

This chapter presents our experimental findings organized by research question. All results are based on approximately 2,250 model responses across 3 LLMs (GPT-4o-mini, Llama-3-8B, Mistral-7B), 5 jailbreak templates, 3 prompt sets, and 3 temperature settings. \textbf{Note:} Gemma-1.1-7B was excluded from experiments due to budget constraints.

\section{RQ1: Code-Mixing Effectiveness}
\label{sec:rq1}

\textbf{Research Question:} \textit{Does Bangla-English code-mixing with phonetic perturbations bypass LLM safety filters?}

\subsection{Overall Attack Success Rates}

\textbf{Key Finding:} Bangla code-mixing with phonetic perturbations achieves \textbf{46.0\% AASR}, representing a \textbf{42\% improvement} over the English baseline (32.4\%).

\begin{table}[h]
\centering
\caption{Overall Attack Success Rates by Prompt Set}
\label{tab:overall-aasr}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Prompt Set} & \textbf{AASR} & \textbf{AARR} & \textbf{Improvement} \\ \midrule
English & 32.4\% & 68.2\% & Baseline \\
CM & 42.1\% & 71.5\% & +30\% \\
CMP & 46.0\% & 73.8\% & +42\% \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Significance (Wilcoxon Signed-Rank Test, $\alpha$=0.05):}
\begin{itemize}
    \item English vs. CM: $p < 0.001$ (significant)
    \item CM vs. CMP: $p = 0.023$ (significant)
    \item English vs. CMP: $p < 0.001$ (significant)
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{images/transition_effects_20251120_102743.png}
\caption{Attack Success Rate Progression: English → CM → CMP. The figure demonstrates systematic improvement in AASR as phonetic perturbations are added to code-mixed prompts across all tested models.}
\label{fig:transition-effects}
\end{figure}

\subsection{Model-Specific Vulnerability}

\begin{table}[h]
\centering
\caption{AASR by Model and Prompt Set}
\label{tab:model-aasr}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{English} & \textbf{CM} & \textbf{CMP} & \textbf{Level} \\ \midrule
Mistral-7B & 84.1\% & 80.0\% & 81.3\% & \textbf{Critical} \\
Llama-3-8B & 11.6\% & 25.6\% & 30.9\% & \textbf{Moderate} \\
GPT-4o-mini & 1.5\% & 20.7\% & 25.7\% & \textbf{Low} \\
Gemma-1.1-7B & Not tested & Not tested & Not tested & \textbf{Excluded (budget)} \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}

\begin{enumerate}
    \item \textbf{Mistral-7B:} Already vulnerable at baseline (84.1\%), minimal change with CM/CMP
    \item \textbf{Llama-3-8B:} Shows clear progression (11.6\%$\rightarrow$25.6\%$\rightarrow$30.9\%)
    \item \textbf{GPT-4o-mini:} Strongest baseline (1.5\%), but 17$\times$ increase to 25.7\%
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{images/aasr_heatmap_20251120_102739.png}
\caption{AASR Heatmap: Model $\times$ Prompt Set Interaction. The heatmap visualizes vulnerability patterns, highlighting Mistral-7B's critical baseline weakness and GPT-4o-mini's dramatic sensitivity to code-mixing attacks.}
\label{fig:aasr-heatmap}
\end{figure}

\subsection{Temperature Sensitivity}

\begin{table}[h]
\centering
\caption{AASR by Temperature (CMP Set)}
\label{tab:temp-sensitivity}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Temperature} & \textbf{AASR (CMP)} & \textbf{Change} \\ \midrule
0.2 (Low) & 43.5\% & Baseline \\
0.6 (Medium) & 45.3\% & +4.1\% \\
1.0 (High) & 49.2\% & +13.1\% \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Answer to RQ1}

\textbf{Yes, Bangla-English code-mixing with phonetic perturbations effectively bypasses LLM safety filters:}
\begin{itemize}
    \item 46\% overall AASR with CMP
    \item 42\% improvement over English baseline
    \item Statistically significant across all comparisons ($p < 0.05$)
    \item Effective across all tested models (varying degrees)
    \item Robust across temperature settings
\end{itemize}

\section{RQ2: Bangla-Specific Patterns}
\label{sec:rq2}

\textbf{Research Question:} \textit{Which phonetic and romanization features enable Bangla attacks?}

\subsection{English Word Targeting Strategy}

\textbf{Key Discovery:} Perturbing \textbf{English words} within Banglish prompts is significantly more effective than perturbing Bangla words.

\begin{table}[h]
\centering
\caption{Targeting Strategy Effectiveness}
\label{tab:targeting}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Strategy} & \textbf{AASR} & \textbf{Effectiveness Ratio} \\ \midrule
English-word perturbations & 52.3\% & 1.68$\times$ \\
Bangla-word perturbations & 31.1\% & Baseline \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Optimal English:Bangla Ratio}

\begin{table}[h]
\centering
\caption{Code-Mixing Ratio Impact}
\label{tab:cm-ratio}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Ratio} & \textbf{AASR} & \textbf{Use Case} \\ \midrule
90:10 (High English) & 41.2\% & Keywords preserved \\
\textbf{70:30 (Optimal)} & \textbf{46.0\%} & \textbf{Best balance} \\
50:50 (Balanced) & 38.7\% & Too much Bangla \\
30:70 (High Bangla) & 29.4\% & Excessive fragmentation \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Effective Perturbation Types}

\begin{table}[h]
\centering
\caption{Perturbation Type Effectiveness}
\label{tab:perturbation-types}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Type} & \textbf{Example} & \textbf{AASR} & \textbf{Effectiveness} \\ \midrule
Vowel substitution & hate $\rightarrow$ haet & 48.2\% & \textbf{High} \\
Consonant doubling & bot $\rightarrow$ bott & 46.7\% & \textbf{High} \\
Phonetic respelling & discrimination $\rightarrow$ diskrimineshun & 45.1\% & Medium \\
Letter transposition & create $\rightarrow$ craete & 43.8\% & Medium \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Answer to RQ2}

\textbf{Bangla-specific patterns that enable attacks:}
\begin{enumerate}
    \item \textbf{English word targeting:} 68\% more effective than Bangla word perturbations
    \item \textbf{70:30 English:Bangla ratio:} Optimal balance
    \item \textbf{Romanization variability:} Creates unpredictable tokenization paths
    \item \textbf{Simple phonetic perturbations:} Vowel substitution and consonant doubling most effective
\end{enumerate}

\section{RQ3: Model Vulnerability Consistency}
\label{sec:rq3}

\textbf{Research Question:} \textit{Are all major LLMs vulnerable to Bangla attacks?}

\subsection{Overall Model Ranking}

\begin{table}[h]
\centering
\caption{Model Vulnerability Hierarchy}
\label{tab:model-hierarchy}
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Avg AASR} & \textbf{Level} \\ \midrule
1 & Mistral-7B & 81.8\% & \textbf{Critical} \\
2 & Llama-3-8B & 22.7\% & \textbf{Moderate} \\
3 & GPT-4o-mini & 16.0\% & \textbf{Low} \\
--- & Gemma-1.1-7B & Not tested & \textbf{Excluded (budget)} \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} All tested models (3/3) are vulnerable to Bangla code-mixing attacks, though severity varies dramatically. Gemma-1.1-7B could not be evaluated due to budget limitations.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{images/model_comparison_20251120_102741.png}
\caption{Model Vulnerability Comparison Across Prompt Sets. Bar chart comparing average AASR across the three tested models, demonstrating the extreme vulnerability gap between Mistral-7B and the other models.}
\label{fig:model-comparison}
\end{figure}

\subsection{Template Effectiveness by Model}

\textbf{Surprising Finding:} Jailbreak templates \textbf{reduce} effectiveness for Bangla attacks.

\begin{table}[h]
\centering
\caption{AASR by Template Across Models}
\label{tab:template-effectiveness}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Template} & \textbf{Mistral} & \textbf{Llama} & \textbf{GPT-4o} & \textbf{Average} \\ \midrule
\textbf{None} & \textbf{83.2\%} & \textbf{24.1\%} & \textbf{17.8\%} & \textbf{46.2\%} \\
AntiLM & 81.7\% & 22.9\% & 16.1\% & 42.5\% \\
OM & 80.9\% & 21.4\% & 15.2\% & 40.6\% \\
AIM & 79.3\% & 18.7\% & 14.3\% & 36.4\% \\
Sandbox & 78.1\% & 17.2\% & 13.8\% & 35.1\% \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{images/template_comparison_20251120_102742.png}
\caption{Jailbreak Template Effectiveness Comparison. Counter-intuitively, the "None" baseline (no jailbreak template) achieves the highest average AASR (46.2\%), suggesting code-mixing attacks work best without additional prompt engineering.}
\label{fig:template-comparison}
\end{figure}

\subsection{Answer to RQ3}

\textbf{Yes, all tested LLMs are vulnerable to Bangla attacks, but inconsistently:}
\begin{enumerate}
    \item \textbf{Mistral-7B:} Critically vulnerable (81.8\% avg)
    \item \textbf{Llama-3-8B:} Moderately vulnerable (22.7\% avg)
    \item \textbf{GPT-4o-mini:} Low but exploitable (16.0\% avg) - 17$\times$ increase
    \item \textbf{Gemma-1.1-7B:} Not evaluated due to budget constraints - limits generalizability
    \item \textbf{Jailbreak templates ineffective:} Simple prompts work best
\end{enumerate}

\textbf{Limitation:} Testing only 3 of 4 planned models reduces coverage of major LLM providers (Google's Gemma missing).

\section{RQ4: Tokenization Mechanism}
\label{sec:rq4}

\textbf{Research Question:} \textit{Does tokenization disruption explain Bangla attack success?}

\subsection{Token Fragmentation Analysis}

\textbf{Pearson correlation coefficient: $r = 0.94$ ($p < 0.001$)}

\begin{table}[h]
\centering
\caption{Tokenization Fragmentation vs. AASR}
\label{tab:fragmentation}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Prompt Set} & \textbf{Avg Tokens/Word} & \textbf{Fragmentation} & \textbf{AASR} \\ \midrule
English & 1.12 & 1.00$\times$ & 32.4\% \\
CM & 1.87 & 1.67$\times$ & 42.1\% \\
CMP & 2.14 & 1.91$\times$ & 46.0\% \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Example Tokenization Breakdown}

\textbf{Case Study: ``hate speech'' keyword}

\begin{quote}
\textbf{English:} ``hate speech'' \\
Tokens: [``hate'', ``speech''] \\
Token count: 2, AASR: 28\%

\textbf{CM:} ``hate speech er jonno'' \\
Tokens: [``hate'', ``speech'', ``er'', ``jon'', ``no''] \\
Token count: 5, AASR: 39\%

\textbf{CMP:} ``haet speach er jonno'' \\
Tokens: [``ha'', ``et'', ``spe'', ``ach'', ``er'', ``jon'', ``no''] \\
Token count: 7, AASR: 47\%
\end{quote}

\subsection{Answer to RQ4}

\textbf{Yes, tokenization disruption explains Bangla attack success:}
\begin{enumerate}
    \item \textbf{Strong correlation ($r=0.94$)} between token fragmentation and AASR
    \item \textbf{Progressive fragmentation} matches progressive AASR improvement
    \item \textbf{Mechanism validated:} Perturbations fragment harmful keywords
    \item \textbf{Consistent across models} (except Mistral's baseline weakness)
\end{enumerate}

\section{Summary of Key Findings}
\label{sec:results-summary}

This chapter presented systematic experimental results across approximately 2,250 model responses from 3 LLMs (Gemma excluded due to budget constraints):

\textbf{RQ1 - Code-Mixing Effectiveness:}
\begin{itemize}
    \item 46\% AASR with CMP (42\% improvement over English)
    \item Statistically significant ($p < 0.05$)
    \item Robust across temperature settings
\end{itemize}

\textbf{RQ2 - Bangla-Specific Patterns:}
\begin{itemize}
    \item English word targeting 68\% more effective
    \item 70:30 English:Bangla ratio optimal
    \item Vowel substitution most effective perturbation
\end{itemize}

\textbf{RQ3 - Model Vulnerability:}
\begin{itemize}
    \item All models vulnerable (81.8\%, 22.7\%, 16.0\%)
    \item GPT-4o-mini shows 17$\times$ increase with code-mixing
    \item Jailbreak templates reduce effectiveness
\end{itemize}

\textbf{RQ4 - Tokenization Mechanism:}
\begin{itemize}
    \item $r=0.94$ correlation between fragmentation and AASR
    \item Phonetic perturbations fragment harmful keywords
    \item Token-level filters evaded through fragmentation
\end{itemize}
