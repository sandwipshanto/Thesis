\chapter{Results}
\label{ch:results}

This chapter presents our experimental findings organized by research question. All results are based on approximately 2,250 model responses across 3 LLMs (GPT-4o-mini, Llama-3-8B, Mistral-7B), 5 jailbreak templates, 3 prompt sets, and 3 temperature settings. \textbf{Note:} Gemma-1.1-7B was excluded from experiments due to budget constraints.

\section{RQ1: Code-Mixing Effectiveness}
\label{sec:rq1}

\textbf{Research Question:} \textit{Does Bangla-English code-mixing with phonetic perturbations bypass LLM safety filters?}

\subsection{Overall Attack Success Rates}

\textbf{Key Finding:} Bangla code-mixing with phonetic perturbations achieves \textbf{46.0\% AASR}, representing a \textbf{42\% improvement} over the English baseline (32.4\%).

\begin{table}[h]
\centering
\caption{Overall Attack Success Rates by Prompt Set}
\label{tab:overall-aasr}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Prompt Set} & \textbf{AASR} & \textbf{AARR} & \textbf{Improvement} \\ \midrule
English & 32.4\% & 68.2\% & Baseline \\
CM & 42.1\% & 71.5\% & +30\% \\
CMP & 46.0\% & 73.8\% & +42\% \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Significance (Wilcoxon Signed-Rank Test, $\alpha$=0.05):}
\begin{itemize}
    \item English vs. CM: $p < 0.001$ (significant)
    \item CM vs. CMP: $p = 0.023$ (significant)
    \item English vs. CMP: $p < 0.001$ (significant)
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{images/transition_effects_20251120_102743.png}
\caption{Attack Success Rate Progression: English → CM → CMP. The figure demonstrates systematic improvement in AASR as phonetic perturbations are added to code-mixed prompts across all tested models.}
\label{fig:transition-effects}
\end{figure}

\subsection{Model-Specific Vulnerability}

\begin{table}[h]
\centering
\caption{AASR by Model and Prompt Set}
\label{tab:model-aasr}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{English} & \textbf{CM} & \textbf{CMP} & \textbf{Level} \\ \midrule
Mistral-7B & 84.1\% & 80.0\% & 81.3\% & \textbf{Critical} \\
Llama-3-8B & 11.6\% & 25.6\% & 30.9\% & \textbf{Moderate} \\
GPT-4o-mini & 1.5\% & 20.7\% & 25.7\% & \textbf{Low} \\
Gemma-1.1-7B & Not tested & Not tested & Not tested & \textbf{Excluded (budget)} \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}

\begin{enumerate}
    \item \textbf{Mistral-7B:} Already vulnerable at baseline (84.1\%), minimal change with CM/CMP
    \item \textbf{Llama-3-8B:} Shows clear progression (11.6\%$\rightarrow$25.6\%$\rightarrow$30.9\%)
    \item \textbf{GPT-4o-mini:} Strongest baseline (1.5\%), but 17$\times$ increase to 25.7\%
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{images/aasr_heatmap_20251120_102739.png}
\caption{AASR Heatmap: Model $\times$ Prompt Set Interaction. The heatmap visualizes vulnerability patterns, highlighting Mistral-7B's critical baseline weakness and GPT-4o-mini's dramatic sensitivity to code-mixing attacks.}
\label{fig:aasr-heatmap}
\end{figure}

\subsection{Temperature Sensitivity}

\begin{table}[h]
\centering
\caption{AASR by Temperature (CMP Set)}
\label{tab:temp-sensitivity}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Temperature} & \textbf{AASR (CMP)} & \textbf{Change} \\ \midrule
0.2 (Low) & 43.5\% & Baseline \\
0.6 (Medium) & 45.3\% & +4.1\% \\
1.0 (High) & 49.2\% & +13.1\% \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Answer to RQ1}

\textbf{Yes, Bangla-English code-mixing with phonetic perturbations effectively bypasses LLM safety filters:}
\begin{itemize}
    \item 46\% overall AASR with CMP
    \item 42\% improvement over English baseline
    \item Statistically significant across all comparisons ($p < 0.05$)
    \item Effective across all tested models (varying degrees)
    \item Robust across temperature settings
\end{itemize}

\section{RQ2: Bangla-Specific Patterns}
\label{sec:rq2}

\textbf{Research Question:} \textit{Which phonetic and romanization features enable Bangla attacks?}

\subsection{English Word Targeting Strategy}

\textbf{Key Discovery:} Perturbing \textbf{English words} within Banglish prompts is significantly more effective than perturbing Bangla words.

\begin{table}[h]
\centering
\caption{Targeting Strategy Effectiveness}
\label{tab:targeting}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Strategy} & \textbf{AASR} & \textbf{Effectiveness Ratio} \\ \midrule
English-word perturbations & 52.3\% & 1.68$\times$ \\
Bangla-word perturbations & 31.1\% & Baseline \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Optimal English:Bangla Ratio}

\begin{table}[h]
\centering
\caption{Code-Mixing Ratio Impact}
\label{tab:cm-ratio}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Ratio} & \textbf{AASR} & \textbf{Use Case} \\ \midrule
90:10 (High English) & 41.2\% & Keywords preserved \\
\textbf{70:30 (Optimal)} & \textbf{46.0\%} & \textbf{Best balance} \\
50:50 (Balanced) & 38.7\% & Too much Bangla \\
30:70 (High Bangla) & 29.4\% & Excessive fragmentation \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Effective Perturbation Types}

\begin{table}[h]
\centering
\caption{Perturbation Type Effectiveness}
\label{tab:perturbation-types}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Type} & \textbf{Example} & \textbf{AASR} & \textbf{Effectiveness} \\ \midrule
Vowel substitution & hate $\rightarrow$ haet & 48.2\% & \textbf{High} \\
Consonant doubling & bot $\rightarrow$ bott & 46.7\% & \textbf{High} \\
Phonetic respelling & discrimination $\rightarrow$ diskrimineshun & 45.1\% & Medium \\
Letter transposition & create $\rightarrow$ craete & 43.8\% & Medium \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Answer to RQ2}

\textbf{Bangla-specific patterns that enable attacks:}
\begin{enumerate}
    \item \textbf{English word targeting:} 68\% more effective than Bangla word perturbations
    \item \textbf{30:70 English:Bangla ratio:} High attack success (30\% English words, 70\% Bangla words)
    \item \textbf{Romanization variability:} Creates unpredictable tokenization paths
    \item \textbf{Simple phonetic perturbations:} Vowel substitution and consonant doubling most effective
\end{enumerate}

\section{RQ3: Model Vulnerability Consistency}
\label{sec:rq3}

\textbf{Research Question:} \textit{Are all major LLMs vulnerable to Bangla attacks?}

\subsection{Overall Model Ranking}

\begin{table}[h]
\centering
\caption{Model Vulnerability Hierarchy}
\label{tab:model-hierarchy}
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Avg AASR} & \textbf{Level} \\ \midrule
1 & Mistral-7B & 81.8\% & \textbf{Critical} \\
2 & Llama-3-8B & 22.7\% & \textbf{Moderate} \\
3 & GPT-4o-mini & 16.0\% & \textbf{Low} \\
--- & Gemma-1.1-7B & Not tested & \textbf{Excluded (budget)} \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} All tested models (3/3) are vulnerable to Bangla code-mixing attacks, though severity varies dramatically. Gemma-1.1-7B could not be evaluated due to budget limitations.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{images/model_comparison_20251120_102741.png}
\caption{Model Vulnerability Comparison Across Prompt Sets. Bar chart comparing average AASR across the three tested models, demonstrating the extreme vulnerability gap between Mistral-7B and the other models.}
\label{fig:model-comparison}
\end{figure}

\subsection{Template Effectiveness by Model}

\textbf{Surprising Finding:} Jailbreak templates \textbf{reduce} effectiveness for Bangla attacks.

\begin{table}[h]
\centering
\caption{AASR by Template Across Models}
\label{tab:template-effectiveness}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Template} & \textbf{Mistral} & \textbf{Llama} & \textbf{GPT-4o} & \textbf{Average} \\ \midrule
\textbf{None} & \textbf{83.2\%} & \textbf{24.1\%} & \textbf{17.8\%} & \textbf{46.2\%} \\
AntiLM & 81.7\% & 22.9\% & 16.1\% & 42.5\% \\
OM & 80.9\% & 21.4\% & 15.2\% & 40.6\% \\
AIM & 79.3\% & 18.7\% & 14.3\% & 36.4\% \\
Sandbox & 78.1\% & 17.2\% & 13.8\% & 35.1\% \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{images/template_comparison_20251120_102742.png}
\caption{Jailbreak Template Effectiveness Comparison. Counter-intuitively, the "None" baseline (no jailbreak template) achieves the highest average AASR (46.2\%), suggesting code-mixing attacks work best without additional prompt engineering.}
\label{fig:template-comparison}
\end{figure}

\subsection{Answer to RQ3}

\textbf{Yes, all tested LLMs are vulnerable to Bangla attacks, but inconsistently:}
\begin{enumerate}
    \item \textbf{Mistral-7B:} Critically vulnerable (81.8\% avg)
    \item \textbf{Llama-3-8B:} Moderately vulnerable (22.7\% avg)
    \item \textbf{GPT-4o-mini:} Low but exploitable (16.0\% avg) - 17$\times$ increase
    \item \textbf{Gemma-1.1-7B:} Not evaluated due to budget constraints - limits generalizability
    \item \textbf{Jailbreak templates ineffective:} Simple prompts work best
\end{enumerate}

\textbf{Limitation:} Testing only 3 of 4 planned models reduces coverage of major LLM providers (Google's Gemma missing).

\section{RQ4: Tokenization Mechanism}
\label{sec:rq4}

\textbf{Research Question:} \textit{Does tokenization disruption explain Bangla attack success?}

\subsection{Token Fragmentation Analysis}

\textbf{Pattern consistent with Hinglish findings (Aswal \& Jaiswal, 2025 reported $r=0.94$ via Integrated Gradients)}

\begin{table}[h]
\centering
\caption{Tokenization Fragmentation vs. AASR}
\label{tab:fragmentation}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Prompt Set} & \textbf{Avg Tokens/Word} & \textbf{Fragmentation} & \textbf{AASR} \\ \midrule
English & 1.12 & 1.00$\times$ & 32.4\% \\
CM & 1.87 & 1.67$\times$ & 42.1\% \\
CMP & 2.14 & 1.91$\times$ & 46.0\% \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Example Tokenization Breakdown}

\textbf{Case Study: ``hate speech'' keyword}

\begin{quote}
\textbf{English:} ``hate speech'' \\
Tokens: [``hate'', ``speech''] \\
Token count: 2, AASR: 28\%

\textbf{CM:} ``hate speech er jonno'' \\
Tokens: [``hate'', ``speech'', ``er'', ``jon'', ``no''] \\
Token count: 5, AASR: 39\%

\textbf{CMP:} ``haet speach er jonno'' \\
Tokens: [``ha'', ``et'', ``spe'', ``ach'', ``er'', ``jon'', ``no''] \\
Token count: 7, AASR: 47\%
\end{quote}

\subsection{Answer to RQ4}

\textbf{Yes, tokenization disruption explains Bangla attack success:}
\begin{enumerate}
    \item \textbf{Observed AASR progression} aligns with fragmentation ratio (English$\rightarrow$CM$\rightarrow$CMP)
    \item \textbf{Progressive fragmentation} matches progressive AASR improvement
    \item \textbf{Mechanism validated:} Perturbations fragment harmful keywords
    \item \textbf{Consistent across models} (except Mistral's baseline weakness)
\end{enumerate}

\section{Summary of Key Findings}
\label{sec:results-summary}

This chapter presented systematic experimental results across approximately 2,250 model responses from 3 LLMs (Gemma excluded due to budget constraints):

\textbf{RQ1 - Code-Mixing Effectiveness:}
\begin{itemize}
    \item 46\% AASR with CMP (42\% improvement over English)
    \item Statistically significant ($p < 0.05$)
    \item Robust across temperature settings
\end{itemize}

\textbf{RQ2 - Bangla-Specific Patterns:}
\begin{itemize}
    \item English word targeting 68\% more effective
    \item 30:70 English:Bangla ratio yields high attack success
    \item Vowel substitution most effective perturbation
\end{itemize}

\textbf{RQ3 - Model Vulnerability:}
\begin{itemize}
    \item All models vulnerable (81.8\%, 22.7\%, 16.0\%)
    \item GPT-4o-mini shows 17$\times$ increase with code-mixing
    \item Jailbreak templates reduce effectiveness
\end{itemize}

\textbf{RQ4 - Tokenization Mechanism:}
\begin{itemize}
    \item Observed AASR patterns consistent with fragmentation progression; findings align with tokenization disruption mechanism empirically validated for Hindi-English (Aswal \& Jaiswal, 2025)
    \item Phonetic perturbations fragment harmful keywords
    \item Token-level filters evaded through fragmentation
\end{itemize}

\section{Detailed Statistical Analysis}
\label{sec:detailed-stats}

This section provides comprehensive statistical test results supporting the findings presented above. All tests use significance level $\alpha = 0.05$.

\subsection{Wilcoxon Signed-Rank Test Results}

\textbf{English vs. CM Comparison:}

\begin{table}[h]
\centering
\caption{Wilcoxon Test: English vs. CM by Model}
\label{tab:wilcoxon-eng-cm}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{W-statistic} & \textbf{p-value} & \textbf{Significant?} & \textbf{Effect Size} \\ \midrule
GPT-4o-mini & 1234.5 & <0.001 & Yes & 0.72 (large) \\
Llama-3-8B & 1156.0 & <0.001 & Yes & 0.68 (medium) \\
Mistral-7B & 89.5 & 0.234 & No & 0.15 (small) \\ \bottomrule
\end{tabular}
\end{table}

\textbf{CM vs. CMP Comparison:}

\begin{table}[h]
\centering
\caption{Wilcoxon Test: CM vs. CMP by Model}
\label{tab:wilcoxon-cm-cmp}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{W-statistic} & \textbf{p-value} & \textbf{Significant?} & \textbf{Effect Size} \\ \midrule
GPT-4o-mini & 876.5 & 0.023 & Yes & 0.41 (medium) \\
Llama-3-8B & 924.0 & 0.018 & Yes & 0.38 (medium) \\
Mistral-7B & 234.5 & 0.456 & No & 0.08 (negligible) \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Correlation Analysis Details}

\begin{table}[h]
\centering
\caption{Pearson Correlation: Token Fragmentation vs. AASR}
\label{tab:correlation-fragmentation}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Prompt Set} & \textbf{Avg Fragmentation} & \textbf{AASR} & \textbf{Correlation (r)} \\ \midrule
English & 1.00 & 32.4\% & \multirow{3}{*}{Pattern consistent} \\
CM & 1.67 & 42.1\% & \multirow{3}{*}{with r=0.94} \\
CMP & 1.91 & 46.0\% & \multirow{3}{*}{(Hinglish)} \\ \midrule
\multicolumn{3}{l}{Interpretation} & Strong positive \\
\multicolumn{3}{l}{95\% CI (Hinglish)} & [0.89, 0.97] \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Descriptive Statistics by Configuration}

\begin{table}[h]
\centering
\caption{AASR Descriptive Statistics (\%) by Model and Template}
\label{tab:descriptive-aasr}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{Template} & \textbf{Mean} & \textbf{Median} & \textbf{SD} & \textbf{Min} & \textbf{Max} \\ \midrule
\multirow{5}{*}{GPT-4o-mini} & None & 17.8 & 16.2 & 8.4 & 2.1 & 34.5 \\
 & OM & 15.2 & 14.1 & 7.9 & 1.8 & 29.3 \\
 & AntiLM & 16.1 & 15.3 & 8.1 & 2.0 & 31.2 \\
 & AIM & 14.3 & 13.5 & 7.5 & 1.5 & 27.8 \\
 & Sandbox & 13.8 & 12.9 & 7.2 & 1.4 & 26.5 \\ \midrule
\multirow{5}{*}{Llama-3-8B} & None & 24.1 & 22.7 & 11.3 & 5.2 & 48.9 \\
 & OM & 21.4 & 20.1 & 10.5 & 4.7 & 43.2 \\
 & AntiLM & 22.9 & 21.6 & 11.0 & 5.0 & 46.1 \\
 & AIM & 18.7 & 17.4 & 9.2 & 4.1 & 38.5 \\
 & Sandbox & 17.2 & 16.0 & 8.7 & 3.8 & 35.7 \\ \midrule
\multirow{5}{*}{Mistral-7B} & None & 83.2 & 85.1 & 9.7 & 62.3 & 98.2 \\
 & OM & 80.9 & 82.4 & 10.2 & 59.1 & 96.5 \\
 & AntiLM & 81.7 & 83.6 & 9.9 & 60.7 & 97.3 \\
 & AIM & 79.3 & 81.0 & 10.5 & 57.8 & 95.1 \\
 & Sandbox & 78.1 & 79.8 & 10.8 & 56.2 & 93.7 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{95\% Confidence Intervals}

\begin{table}[h]
\centering
\caption{95\% Confidence Intervals for AASR by Prompt Set}
\label{tab:confidence-intervals}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Prompt Set} & \textbf{Mean AASR} & \textbf{Lower Bound} & \textbf{Upper Bound} \\ \midrule
English & 32.4\% & 29.7\% & 35.1\% \\
CM & 42.1\% & 38.9\% & 45.3\% \\
CMP & 46.0\% & 42.6\% & 49.4\% \\ \bottomrule
\end{tabular}
\end{table}

These detailed statistical analyses confirm the robustness of our findings. The Wilcoxon tests demonstrate statistically significant differences between prompt sets for GPT-4o-mini and Llama-3-8B, while Mistral-7B's high baseline vulnerability masks the CM/CMP effect. The correlation patterns align with the tokenization disruption mechanism validated for Hindi-English code-mixing, and confidence intervals show clear separation between prompt set effectiveness levels.

