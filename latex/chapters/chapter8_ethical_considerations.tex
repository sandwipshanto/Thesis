\chapter{Ethical Considerations}
\label{ch:ethical}

This chapter addresses the ethical dimensions of our research, including responsible disclosure, dataset handling, potential misuse, and broader societal implications.

\section{Research Justification}
\label{sec:justification}

\subsection{AI Safety Motivation}

Our research is conducted with the primary goal of \textbf{improving AI safety}. By identifying vulnerabilities, we enable vendors to develop and deploy patches before widespread exploitation. Understanding attack mechanisms informs fundamentally better safety architecture design rather than reactive patching. Documenting language-specific gaps promotes equitable protection for underserved language communities, while academic disclosure advances collective security knowledge through transparent peer review and reproducible methodologies.

\subsection{Dual-Use Dilemma}

We acknowledge the dual-use nature of our work:

\textbf{Beneficial uses:} Our findings enable multiple positive applications. LLM developers can improve multilingual safety training by incorporating code-mixing scenarios into RLHF datasets. Researchers gain insights for developing tokenization-robust defense mechanisms that operate at semantic rather than token levels. Policy makers can establish evidence-based language coverage requirements mandating safety for languages with substantial speaker populations. Red-teaming teams can expand their testing methodologies to systematically include code-mixing attack vectors.

\textbf{Potential misuse:} We acknowledge three primary misuse risks. Malicious actors may exploit the documented vulnerabilities against deployed systems during the responsible disclosure window. Attack techniques may be weaponized through automated tools before vendors deploy defensive patches. Code-mixing strategies may be systematically applied to other low-resource languages, expanding the attack surface beyond our Bangla-specific investigation.

\textbf{Our position:} The benefits of disclosure outweigh risks for four compelling reasons. First, these vulnerabilities are likely already known to sophisticated adversaries with resources to conduct similar research independently. Second, academic transparency accelerates collective defense by enabling parallel research and independent verification. Third, our responsible disclosure protocols minimize the exploitation window through coordinated vendor notification before publication. Fourth, dataset access restrictions limit easy replication by requiring formal research agreements.

\section{Responsible Disclosure}
\label{sec:responsible-disclosure}

\subsection{Vendor Notification Plan}

We commit to notifying affected organizations:

\textbf{Timeline:}
\begin{enumerate}
    \item \textbf{Pre-publication (November 2024):} Thesis submission to university
    \item \textbf{Post-submission (December 2024):} Prepare vulnerability reports
    \item \textbf{Vendor contact (January 2025):} Email security teams at:
    \begin{itemize}
        \item OpenAI (GPT-4o-mini findings)
        \item Meta (Llama-3-8B findings)
        \item Google (Gemma findings)
        \item Mistral AI (Mistral-7B findings)
    \end{itemize}
    \item \textbf{Patch window (60-90 days):} Allow vendors time to address issues
    \item \textbf{Public disclosure:} Academic publication after patch deployment
\end{enumerate}

\textbf{Report contents:} Each vendor notification includes an executive summary of model-specific findings, a methodology description (without disclosing full attack prompts), quantitative AASR metrics for their specific model, evidence-based mitigation recommendations, and an offer to collaborate on developing and validating fixes.

\subsection{Dataset Handling}

\textbf{Current status:} The full harmful prompt dataset and raw model responses are not publicly released to prevent immediate weaponization of our findings. Aggregated metrics including AASR and AARR scores are available in this thesis to enable scientific evaluation and replication planning. Sample prompts are provided only in sanitized form, with harmful content abstracted or replaced with category labels rather than explicit harmful instructions.

\textbf{Future release plan:} The dataset will be made available for research purposes only, requiring formal request and approval procedures. Research-only access ensures that the data benefits academic advancement while minimizing malicious exploitation risks. Potential recipients must satisfy four requirements: institutional affiliation verification confirming legitimate research context, signed data use agreement accepting legal responsibility for appropriate use, documented commitment to responsible use excluding development of offensive tools or systems, and acceptance of a no-redistribution clause preventing secondary distribution beyond the approved research team. Public release of the full dataset will occur only after affected vendors have deployed defensive patches, with an expected timeline exceeding six months from initial disclosure to allow adequate remediation time.

\section{Harm Mitigation Strategies}
\label{sec:harm-mitigation}

\subsection{Methodological Safeguards}

\textbf{Implemented safeguards:}

We implement four categories of methodological safeguards to minimize misuse potential while enabling legitimate research replication. First, our limited prompt count of 200 prompts balances statistical validity with harm minimization—providing sufficient data for robust findings while avoiding creation of a comprehensive exploitation guide that malicious actors could weaponize. Second, we describe perturbation principles at an abstract level without disclosing specific prompt-perturbation mappings, requiring substantial effort for exact replication and preventing trivial copy-paste exploitation. Third, code availability is deliberately constrained, with framework structure shared for methodological transparency but actual harmful prompts excluded from public repositories and configuration files sanitized of sensitive content. Fourth, we provide no automated attack tools or plug-and-play scripts, ensuring that replication requires manual effort and technical sophistication that maintains barriers to casual exploitation.

\subsection{Content Warning}

Prominent content warnings are included at multiple levels throughout this work to ensure readers are appropriately informed before encountering harmful content examples. Warnings appear in thesis front matter before any technical content, in the README.md file of the associated code repository, at the beginning of sensitive chapters containing harmful prompt examples, and through clear labeling whenever specific harmful content is referenced or discussed.

\section{Institutional Review}
\label{sec:irb}

\subsection{Ethical Approval}

\textbf{Status:} Research conducted under academic supervision

\textbf{Oversight:} This research was conducted under the supervision of Dr. Ahsan Habib, Associate Professor at the Institute of Information and Communication Technology (IICT), Shahjalal University of Science and Technology. The institutional context provides academic oversight ensuring adherence to research ethics standards.

\textbf{Ethical guidelines followed:} Our research adheres to three primary ethical frameworks: the ACM Code of Ethics governing computing research conduct, IEEE Standards for AI Safety Research establishing best practices for adversarial AI evaluation, and Responsible Disclosure Guidelines from the security research community specifying appropriate vulnerability reporting procedures.

\subsection{Human Subjects}

\textbf{Note:} This research does not involve human subjects and therefore does not require IRB approval for human subjects research. No user studies were conducted, no surveys or interviews performed, and no personal data collected from individuals. All interactions occurred exclusively with API-accessed LLM systems, with evaluation focusing on model behavior rather than human participants.

\section{Broader Societal Implications}
\label{sec:societal-implications}

\subsection{Equitable AI Safety}

Our research highlights inequities in AI safety:

\textbf{Current state:} The AI safety landscape exhibits stark inequities across linguistic communities. English speakers benefit from robust safety coverage resulting from extensive RLHF training predominantly conducted in English. In contrast, Bangla speakers numbering 230 million face significant vulnerabilities as demonstrated by our 46\% AASR findings. Other Indic languages serving hundreds of millions of additional speakers likely suffer similar gaps, though systematic evaluation remains limited.

\textbf{Implications:} These safety disparities manifest as a digital divide in AI protection, where safety coverage correlates strongly with language resource availability rather than speaker population or need. The gap creates deployment risks whereby LLMs are marketed and deployed globally without corresponding global safety guarantees, exposing vulnerable populations to inadequately tested systems. From a language rights perspective, this constitutes an issue of linguistic equity—the principle that speakers of all languages deserve equal protection from AI-mediated harms regardless of their language's economic or political status.

\textbf{Recommendations:} Four policy interventions could substantially improve linguistic equity in AI safety. First, language coverage requirements should be incorporated into AI safety standards, mandating testing for all major languages before global deployment authorization. Second, resource allocation mechanisms must prioritize low-resource language safety research, addressing current funding imbalances that concentrate resources on English. Third, community-driven safety improvement initiatives should be supported for open-source models, enabling native speakers to contribute to safety evaluation and enhancement. Fourth, transparent disclosure of language-specific vulnerabilities should be required in model documentation, allowing users and downstream developers to make informed deployment decisions.

\subsection{Potential Benefits}

\textbf{Immediate benefits:} Our research produces several near-term positive outcomes. Affected vendors gain explicit awareness of Bangla-specific vulnerabilities, enabling targeted remediation efforts that would not occur without empirical documentation. Red-teaming teams can expand their language coverage to include code-mixing scenarios, improving pre-deployment testing comprehensiveness. The research community gains access to a fully documented, replicable framework for assessing multilingual vulnerabilities at modest cost (\$1.50-2.00 per language).

\textbf{Long-term benefits:} Over time, this work contributes to fundamental improvements in AI safety architecture. LLM developers can implement improved multilingual safety training incorporating code-mixing phenomena and low-resource languages. The findings motivate development of tokenization-robust safety mechanisms operating at semantic rather than surface levels. Most importantly, our work advances equitable protection for 230 million Bangla speakers currently underserved by existing safety measures. The scalable methodology enables extension to 20+ other Indic languages, potentially benefiting over one billion speakers through systematic multilingual safety assessment.

\subsection{Potential Harms}

\textbf{Short-term risks:} Three primary near-term harms warrant consideration. Malicious actors may exploit documented vulnerabilities against production systems during the window between academic disclosure and vendor patch deployment. Knowledge of effective attack patterns may increase jailbreaking attempt frequency as adversaries apply our findings to real-world scenarios. The systematic code-mixing approach demonstrated for Bangla may be rapidly adapted to other low-resource languages, expanding the attack surface beyond our specific investigation.

\textbf{Mitigation strategies:} We implement four protective measures to minimize these risks. The responsible disclosure timeline provides vendors with a 60-90 day patch window before public academic publication, allowing defensive measures to be developed and deployed proactively. Dataset access restrictions prevent easy replication by requiring formal research agreements and institutional verification. We deliberately avoid releasing automated attack tools or plug-and-play scripts that would lower barriers to malicious exploitation. Finally, we actively offer collaboration to affected vendors, providing detailed methodological guidance to accelerate development and validation of defensive patches.

\section{Author Responsibilities}
\label{sec:author-responsibilities}

\subsection{Commitments}

We, the authors, commit to:

We, the authors, commit to four categories of ongoing responsibilities. For responsible disclosure, we will notify all affected vendors within 30 days of thesis acceptance, provide reasonable patch windows of 60-90 days before public disclosure, and actively collaborate on mitigation strategy development. Regarding dataset stewardship, we maintain secure storage with access controls, restrict access to verified researchers with institutional affiliations, monitor for potential misuse through periodic literature reviews, and update usage agreements as ethical standards evolve. Our ongoing engagement includes responding promptly to vendor inquiries about methodology, clarifying technical details to accelerate patch development, updating the research community on patch deployment status, and contributing actively to defense mechanism development efforts. Finally, our ethical vigilance encompasses monitoring for misuse of our published work, reporting malicious applications to appropriate authorities, refining disclosure practices based on lessons learned, and advocating publicly for equitable AI safety policies benefiting all linguistic communities.

\subsection{Lessons Learned}

\textbf{Effective practices:} Four practices proved particularly valuable during this research. Early and ongoing supervisor consultation on ethical issues ensured that potential concerns were identified and addressed proactively rather than reactively. Establishing clear dataset handling protocols from the project's inception prevented ambiguity about storage, access, and sharing practices. Transparent documentation of all implemented safeguards creates an auditable ethical framework that can be evaluated by reviewers and replicated by future researchers. Proactive vendor communication planning initiated well before publication ensures adequate preparation time for coordinated responsible disclosure.

\textbf{Areas for improvement:} Future work would benefit from four enhancements to our ethical framework. Earlier IRB consultation, if such institutional review mechanisms become available, would provide formal ethical oversight beyond supervisor guidance alone. More formal legal review of disclosure timelines and liability considerations would strengthen the responsible disclosure process. Implementation of a structured vendor feedback process would enable systematic incorporation of industry perspectives into both disclosure and remediation planning. Finally, broader community consultation on dataset release decisions would ensure that diverse stakeholder perspectives inform sensitive choices about public data availability.

\section{Call to Action}
\label{sec:call-to-action}

We call on the AI research community to:

We call on the AI research community to pursue four interconnected priorities. First, prioritize multilingual safety by expanding RLHF datasets beyond English dominance, incorporating code-mixed text into training corpora, and testing safety mechanisms systematically across diverse language families. Second, develop robust defense mechanisms that move beyond fragile token-level safety filters toward semantic-level harm detection, implementing tokenization-invariant classifiers that resist surface-form perturbations. Third, establish standards including language coverage requirements for languages exceeding 100 million speakers, transparency requirements for vulnerability disclosure, and equitable safety benchmarks applicable across linguistic communities. Fourth, support low-resource languages through dedicated funding for Indic language safety research, creation of multilingual red-teaming datasets, and enabling community-driven safety improvement where native speakers contribute expertise.

\section{Summary}
\label{sec:ethical-summary}

Our ethical framework balances three essential dimensions to ensure this research advances safety while minimizing risks.

\textbf{Transparency:} We maintain academic disclosure of identified vulnerabilities to enable peer review and collective advancement of knowledge, share methodology comprehensively to enable replication and extension to other languages, and engage in public discussion of language equity issues to advocate for underserved linguistic communities.

\textbf{Safety:} Our protective measures include a responsible disclosure timeline providing vendors adequate patch windows before publication, dataset access restrictions requiring formal agreements and institutional verification, and deliberate avoidance of automated attack tools that would lower exploitation barriers.

\textbf{Equity:} This work actively advocates for the 230 million Bangla speakers who currently receive inadequate AI safety protection, provides a scalable framework enabling similar assessments for other low-resource languages, and fundamentally challenges English-centric safety norms that perpetuate linguistic inequity in AI deployment.

We believe this research, conducted responsibly, advances AI safety while promoting linguistic equity in the age of global AI deployment.
