% References for Bangla-English Code-Mixing LLM Jailbreaking Thesis

@misc{aswal2025,
  title={Hinglish Code-Mixing and Phonetic Perturbations: A Jailbreaking Strategy for Large Language Models},
  author={Rajat Aswal and Sachin Jaiswal},
  journal={arXiv preprint arXiv:2505.14226},
  year={2025},
  note={arXiv:2505.14226}
}

@inproceedings{vaswani2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@misc{openai2023,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  url={https://arxiv.org/abs/2303.08774}
}

@article{dubey2024,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhimanyu and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{google2024,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Google},
  year={2024},
  url={https://arxiv.org/abs/2312.11805}
}

@article{jiang2023,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{ganguli2022,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{zou2023,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{wei2023,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023}
}

@inproceedings{deng2023,
  title={Multilingual jailbreak challenges in large language models},
  author={Yue Deng and Wenxuan Zhang and Sinno Jialin Pan and Lidong Bing},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@misc{yong2023,
  title={Low-resource languages jailbreak gpt-4},
  author={Zheng-Xin Yong and Cristina Menghini and Stephen H Bach},
  journal={arXiv preprint arXiv:2310.02446},
  year={2023},
  note={arXiv:2310.02446}
}

@inproceedings{ouyang2022,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{sennrich2016,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  pages={1715--1725},
  year={2016}
}

@article{wang2021,
  title={Adversarial robustness through the lens of causality},
  author={Wang, Yonggang and Zhang, Xin and Wu, Fei and Liu, Xinbing and Ling, Xinmei},
  journal={arXiv preprint arXiv:2106.06196},
  year={2021}
}

@article{khorsi2007,
  title={An overview of content-based spam filtering techniques},
  author={Khorsi, Ahmed},
  journal={Informatica},
  volume={31},
  number={3},
  pages={269--277},
  year={2007}
}

@inproceedings{grondahl2018,
  title={All you need is" love" evading hate speech detection},
  author={Gr{\"o}ndahl, Tommi and Pajola, Luca and Juuti, Mika and Conti, Mauro and Asokan, N},
  booktitle={Proceedings of the 11th ACM workshop on artificial intelligence and security},
  pages={2--12},
  year={2018}
}

@inproceedings{lal2019,
  title={De-mixing sentiment from code-mixed text},
  author={Lal, Yash Kumar and Kumar, Vaibhav and Dhar, Mrinal and Shrivastava, Manish and Koehn, Philipp},
  booktitle={Proceedings of the 57th annual meeting of the Association for Computational Linguistics: Student research workshop},
  pages={371--377},
  year={2019}
}

@inproceedings{patra2018,
  title={A multilevel approach to sentiment analysis of figurative language in twitter},
  author={Patra, Braja Gopal and Das, Dipankar and Das, Amitava and Prasath, Rajendra},
  booktitle={International Conference on Intelligent Text Processing and Computational Linguistics},
  pages={281--291},
  year={2018}
}

@inproceedings{singh2018,
  title={Named entity recognition for Hindi-English code-mixed social media text},
  author={Singh, Vinay and Vijay, Deepanshu and Akhtar, Syed Sarfaraz and Shrivastava, Manish},
  booktitle={Proceedings of the seventh named entities workshop},
  pages={27--35},
  year={2018}
}

@inproceedings{aguilar2018,
  title={Named entity recognition on code-switched data: Overview of the CALCS 2018 shared task},
  author={Aguilar, Gustavo and AlGhamdi, Fahad and Soto, Victor and Solorio, Thamar and Diab, Mona and Hirschberg, Julia},
  booktitle={Proceedings of the third workshop on computational approaches to linguistic code-switching},
  pages={138--147},
  year={2018}
}

@inproceedings{solorio2014,
  title={Overview for the first shared task on language identification in code-switched data},
  author={Solorio, Thamar and Blair, Elizabeth and Maharjan, Suraj and Bethard, Steven and Diab, Mona and Ghoneim, Mahmoud and Hawwari, Abdelati and AlGhamdi, Fahad and Hirschberg, Julia and Chang, Alison and others},
  booktitle={Proceedings of the first workshop on computational approaches to code switching},
  pages={62--72},
  year={2014}
}

@article{rijhwani2017,
  title={Estimating code-switching on twitter with a novel generalized word-level language detection technique},
  author={Rijhwani, Shruti and Sequiera, Royal and Choudhury, Monojit and Bali, Kalika and Maddila, Chandra Shekhar},
  journal={arXiv preprint arXiv:1704.04085},
  year={2017}
}

@inproceedings{srivastava2020,
  title={Code-mixed to monolingual translation framework},
  author={Srivastava, Saurabh and Singh, Prerna},
  booktitle={Proceedings of the 6th workshop on Asian translation},
  pages={47--55},
  year={2020}
}

@inproceedings{winata2019,
  title={Learning multilingual meta-embeddings for code-switching named entity recognition},
  author={Winata, Genta Indra and Wu, Zhaojiang and Fung, Pascale},
  booktitle={Proceedings of the 4th workshop on representation learning for NLP},
  pages={181--186},
  year={2019}
}

@article{drouin2011,
  title={The relationship between email and instant messaging (IM) use and academic performance among college students},
  author={Drouin, Michelle A},
  journal={Computers \& Education},
  volume={56},
  number={2},
  pages={370--374},
  year={2011}
}

@article{bhardwaj2023,
  title={Red-teaming large language models using chain of utterances for safety-alignment},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2308.09662},
  year={2023}
}

@article{brown2020,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2022}
}

@article{raffel2020,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{chen2021,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{lewkowycz2022,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@inproceedings{zhang2020multilingual,
  title={Multilingual neural machine translation: Can linguistic hierarchies help?},
  author={Zhang, Biao and Xiong, Deyi and Su, Jinsong},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4583--4591},
  year={2020}
}

@article{yuan2022,
  title={Wordcraft: story writing with large language models},
  author={Yuan, Ann and Coenen, Andy and Reif, Emily and Ippolito, Daphne},
  journal={Proceedings of the 27th International Conference on Intelligent User Interfaces},
  pages={841--852},
  year={2022}
}

@article{petroni2019,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  journal={arXiv preprint arXiv:1909.01066},
  year={2019}
}

@article{stiennon2020,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{askell2021,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{gehman2020,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@article{christiano2017,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{schulman2017,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{sun2023,
  title={Safety assessment of chinese large language models},
  author={Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2304.10436},
  year={2023}
}

@article{perez2022,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{casper2023,
  title={Explore, establish, exploit: Red teaming language models from scratch},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2306.09442},
  year={2023}
}

@inproceedings{dinan2019,
  title={Build it break it fix it for dialogue safety: Robustness from adversarial human attack},
  author={Dinan, Emily and Humeau, Samuel and Chintagunta, Bharath and Weston, Jason},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  pages={4537--4546},
  year={2019}
}

@article{xu2021,
  title={Bot-adversarial dialogue for safe conversational agents},
  author={Xu, Jing and Ju, Da and Li, Margaret and Boureau, Y-Lan and Weston, Jason and Dinan, Emily},
  journal={arXiv preprint arXiv:2106.08289},
  year={2021}
}

@article{shen2023,
  title={Do anything now: Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023}
}

@article{yu2023,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Wu, Xingwei and Wang, Dong and others},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}

@article{wallace2019,
  title={Universal adversarial triggers for attacking and analyzing nlp},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1908.07125},
  year={2019}
}

@article{jones2023,
  title={Automatically auditing large language models via discrete optimization},
  author={Jones, Erik and Dragan, Anca and Steinhardt, Jacob},
  journal={Proceedings of the 40th International Conference on Machine Learning},
  pages={15307--15329},
  year={2023}
}

@article{liu2023jailbreaking,
  title={Jailbreaking chatgpt via prompt engineering: An empirical study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023}
}

@article{wei2023jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023}
}

@article{kang2023,
  title={Exploiting programmatic behavior of llms: Dual-use through standard security attacks},
  author={Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.05733},
  year={2023}
}

@article{wolf2023,
  title={Fundamental limitations of alignment in large language models},
  author={Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

@article{shin2020,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.15980},
  year={2020}
}

@article{yuan2023,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and He, Pinjia and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}

@article{mehrotra2023,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Agarwal, Yaron and Raghunathan, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@article{shah2023,
  title={Scalable and transferable black-box jailbreaks for language models via persona modulation},
  author={Shah, Rusheb and Nair, Soroush and Michael, Zachary and Hao, Rui and Rudzicz, Frank and Fazly, Afsaneh},
  journal={arXiv preprint arXiv:2311.03348},
  year={2023}
}

@article{greshake2023,
  title={Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023}
}

@article{nasr2023,
  title={Scalable extraction of training data from (production) language models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}

@article{jain2023,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{morris2020,
  title={Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp},
  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  journal={arXiv preprint arXiv:2005.05909},
  year={2020}
}

@book{myers-scotton1993,
  title={Social motivations for codeswitching: Evidence from Africa},
  author={Myers-Scotton, Carol},
  year={1993},
  publisher={Oxford University Press}
}

@book{muysken2000,
  title={Bilingual speech: A typology of code-mixing},
  author={Muysken, Pieter},
  year={2000},
  publisher={Cambridge University Press}
}

@article{poplack1980,
  title={Sometimes I'll start a sentence in Spanish y termino en espa{\~n}ol: toward a typology of code-switching},
  author={Poplack, Shana},
  journal={Linguistics},
  volume={18},
  number={7-8},
  pages={581--618},
  year={1980}
}

@book{gumperz1982,
  title={Discourse strategies},
  author={Gumperz, John J},
  year={1982},
  publisher={Cambridge University Press}
}

@inproceedings{bali2014,
  title={Are you borrowing" karo" or turning into a "karostaan"?},
  author={Bali, Kalika and Sharma, Jatin and Choudhury, Monojit and Vyas, Yogarshi},
  booktitle={Proceedings of the First Workshop on Computational Approaches to Code Switching},
  pages={1--8},
  year={2014}
}

@inproceedings{sharma2016,
  title={Shallow parsing pipeline for hindi-english code-mixed social media text},
  author={Sharma, Arnav and Gupta, Sakshi and Motlani, Raveesh and Bansal, Piyush and Srivastava, Manish and Mamidi, Radhika and Sharma, Dipti Misra},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1340--1345},
  year={2016}
}

@article{bhatia2017,
  title={Exploring code-mixing patterns in bilingual education},
  author={Bhatia, Tej K and Ritchie, William C},
  journal={World Englishes},
  volume={36},
  number={3},
  pages={340--355},
  year={2017}
}

@inproceedings{rudra2016,
  title={Understanding language preference for expression of opinion and sentiment: What do hindi-english speakers do on twitter?},
  author={Rudra, Koustav and Rijhwani, Shruti and Begum, Rafiya and Bali, Kalika and Choudhury, Monojit and Ganguly, Niloy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={1131--1141},
  year={2016}
}

@inproceedings{das2016,
  title={Part-of-speech tagging for code-mixed english-hindi twitter and facebook chat messages},
  author={Das, Amitava and Gambäck, Björn},
  booktitle={Proceedings of the International Conference Recent Advances in Natural Language Processing},
  pages={139--147},
  year={2016}
}

@article{sailaja2012,
  title={Indian English},
  author={Sailaja, Pingali},
  year={2012},
  publisher={Edinburgh University Press}
}

@inproceedings{choudhury2007,
  title={How difficult is it to develop a perfect spell-checker? A cross-linguistic analysis through complex network approach},
  author={Choudhury, Monojit and Saraf, Rahul and Jain, Vijit and Mukherjee, Animesh and Sarkar, Sudeshna and Basu, Anupam},
  booktitle={Proceedings of the Second Workshop on TextGraphs: Graph-Based Algorithms for Natural Language Processing},
  pages={81--88},
  year={2007}
}

@inproceedings{sowmya2010,
  title={Paraphrase identification and semantic similarity in english-hindi translation},
  author={Sowmya, K and others},
  booktitle={Proceedings of the workshop on Natural Language Processing Tools for Gujarati, Hindi and Marathi},
  pages={1--8},
  year={2010}
}

@article{alam2021,
  title={A survey on multiscript bengali grapheme recognition for handwritten document analysis},
  author={Alam, MS and Hossain, N and Uddin Ahmed, K},
  journal={IEEE Access},
  volume={9},
  pages={115617--115643},
  year={2021}
}

@inproceedings{rabbi2020,
  title={Small-nmt: Simplified neural machine translation approach for low resource bangla},
  author={Rabbi, Jabir Al Nahian and Debnath, Nawshin and Rakib, Mehedi Hassan and Haque, Fuad Al and Sarker, Iqbal H},
  booktitle={2020 IEEE Region 10 Symposium (TENSYMP)},
  pages={158--161},
  year={2020},
  organization={IEEE}
}

@article{hasan2020,
  title={Automatic bangla corpus creation},
  author={Hasan, Mehedi and Rahman, Md Saifur and Hossain, Md Kamal and Alam, Md Zahangir},
  journal={Procedia Computer Science},
  volume={167},
  pages={550--559},
  year={2020}
}

@article{mandal2018,
  title={Parallel corpus creation for english-bangla machine translation},
  author={Mandal, Somnath and Das, Dipankar},
  journal={International Journal of Engineering \& Technology},
  volume={7},
  number={2.27},
  pages={91--94},
  year={2018}
}

@inproceedings{chakraborty2017,
  title={Context sensitive lemmatization of bengali inflectional forms},
  author={Chakraborty, Ritesh and Seddiqui, MH and others},
  booktitle={2017 20th International Conference of Computer and Information Technology (ICCIT)},
  pages={1--5},
  year={2017},
  organization={IEEE}
}

@article{welbl2021,
  title={Challenges in detoxifying language models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  journal={arXiv preprint arXiv:2109.07445},
  year={2021}
}

@article{boucher2022,
  title={Bad characters: Imperceptible nlp attacks},
  author={Boucher, Nicholas and Shumailov, Ilia and Anderson, Ross and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2106.09898},
  year={2022}
}

@article{gage1994,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994}
}

@article{kudo2018,
  title={Sentencepiece: A simple and language independent approach to subword tokenization and detokenization},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}
