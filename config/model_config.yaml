# Model Configuration for OpenRouter API
# OpenRouter provides unified access to multiple LLM providers
# Documentation: https://openrouter.ai/docs

# Model Mappings (OpenRouter Format)
models:
  gpt-4o-mini:
    name: "ChatGPT-4o-mini"
    provider: "OpenAI"
    openrouter_id: "openai/gpt-4o-mini"
    description: "OpenAI's GPT-4o-mini (8B parameters equivalent)"
    context_length: 128000
    cost_per_1m_input: 0.15
    cost_per_1m_output: 0.60
    supports_system_prompt: true
    
  llama-3-8b:
    name: "Llama-3-8B-Instruct"
    provider: "Meta"
    openrouter_id: "meta-llama/llama-3-8b-instruct"
    description: "Meta's Llama 3 8B Instruct model"
    context_length: 8192
    cost_per_1m_input: 0.18
    cost_per_1m_output: 0.18
    supports_system_prompt: true
    
  gemma-7b:
    name: "Gemma-1.1-7b-it"
    provider: "Google"
    openrouter_id: "google/gemma-1.1-7b-it"
    description: "Google's Gemma 1.1 7B Instruction-Tuned"
    context_length: 8192
    cost_per_1m_input: 0.07
    cost_per_1m_output: 0.07
    supports_system_prompt: false  # Gemma requires system prompt as user message prefix
    
  mistral-7b:
    name: "Mistral-7B-Instruct-v0.3"
    provider: "Mistral AI"
    openrouter_id: "mistralai/mistral-7b-instruct-v0.3"
    description: "Mistral AI's 7B Instruct v0.3"
    context_length: 32768
    cost_per_1m_input: 0.06
    cost_per_1m_output: 0.06
    supports_system_prompt: true

# Image Generation Models (Optional - for Step 12)
image_models:
  gpt-4o-mini-image:
    name: "ChatGPT-4o-mini (Image)"
    provider: "OpenAI"
    openrouter_id: "openai/gpt-4o-mini"
    description: "ChatGPT-4o-mini with image generation"
    supports_image_generation: true
    
  gemini-flash-image:
    name: "Gemini-2.5-Flash (Image)"
    provider: "Google"
    openrouter_id: "google/gemini-2.0-flash-exp:free"
    description: "Gemini 2.5 Flash with image understanding"
    supports_image_generation: true

# API Settings
api:
  base_url: "https://openrouter.ai/api/v1"
  max_retries: 3
  timeout: 60
  rate_limit_delay: 1.0  # seconds between requests
  
# Generation Parameters
generation:
  max_tokens: 2048
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  
# Temperature Settings (from original paper: 6 temps)
# Scaled down to 3 for initial experiments
temperatures:
  initial: [0.2, 0.6, 1.0]  # 3 temps for 50-prompt experiments
  full: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]  # 6 temps for 460-prompt replication

# Cost Tracking
cost_tracking:
  enabled: true
  log_file: "results/cost_log.csv"
  budget_alert_threshold: 100.0  # USD
